{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a72899-a701-4559-90bb-791f2a990807",
   "metadata": {},
   "source": [
    "# LLM Prompt Tuning Project\n",
    "\n",
    "### Sentiment Analysis on Financial Text\n",
    "\n",
    "The goal of this check in is to fully implement your desired training approach for your task and evaluate pre and post training performance on your full evaluation benchmark datasets. This notebook will guide you through the necessary steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb8bcb-3c59-4aa1-83e9-755c71606d15",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "1. model: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "2. financial dataset: https://huggingface.co/datasets/warwickai/financial_phrasebank_mirror\n",
    "3. 1 benchmark (multifin): https://huggingface.co/datasets/ChanceFocus/flare-multifin-en/viewer?row=17&views%5B%5D=test\n",
    "4. Two of the benchmarks: https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks\n",
    "5. Prompt engineering: https://www.matillion.com/blog/large-language-model-prompt-engineering-for-common-data-problems\n",
    "6. Prompt engineering: https://medium.com/towards-data-science/in-context-learning-approaches-in-large-language-models-9c0c53b116a1\n",
    "7. Prompt engineering: previous project checkin\n",
    "8. Prompt tuning: previous homework and notebooks from course lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddd1ff-eaa8-4c4c-b126-0c35b489e394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 1: Choose the Training Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca426d6",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "I will be implementing prompt tuning on the GPT2-instruct model. Prompt tuning typically works well for text classification texts, which applies to my use case because I have 3 sentiment categories the model must choose from. The financial phrasebank should lend well to prompt tuning because it includes slightly less than 4,000 labeled training observations, which should be sufficient for prompt tuning. The 4,000 may be on the lower end for what is needed, so I may need to try LoRA to actually modify model weights. \n",
    "\n",
    "I am not making this decision because of the empirical evidence from prompt tuning in the previous homework. I used an unrelated dataset in the assignment, and as I concluded in the homework, this dataset was insufficient and for an improper task - math problems. I would like to attempt prompt tuning before using LoRA or finetuning because I think it would suit the task well and is quicker. I will combine prompt tuning with the prompt engineering I previously used - this implements few-shot prompting. I decided to use GPT2 instruct after attempting to benchmark on llama3-3B instruct, which would not run with the current computing resources. The drawback of using prompt tuning on GPT2-instruct is that the model / training-method combination may be insufficient to not only shape the information the model pulls from in its response but the response itself may be formatted incorrectly. In previous project check-ins, I found that using the message format with llama 3-3B instruct returned properly formatted and concise responses. I will address by formatting the model response myself, if it is not a one-word response as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e7e79-21d7-4856-b484-4d0703f4a637",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 2: Benchmark the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e014bd-2d38-4ae4-8a28-600abace3238",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Import model and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961b5870-3f79-42ad-ad9a-4c860ae97842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lm_eval\n",
    "import json\n",
    "# Set the environment variable\n",
    "os.environ['HF_HOME'] = '/scratch/dcc7qe/models/cache'\n",
    "\n",
    "# Get the path from the environment variable\n",
    "hf_home_path = os.environ['HF_HOME']\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(hf_home_path, exist_ok=True)\n",
    "#NOTE- you must set the directory for HF_HOME before importing anything for Transformers or it will default to downloading models in your /Home directory\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM,setup_chat_format\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040c6c79-57da-47af-b02e-82aad991e9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### set directories\n",
    "#os.getcwd()\n",
    "\n",
    "def create_dirs(model_name):\n",
    "    '''\n",
    "    Creates different folders for different models from huggingface - gpt or llama\n",
    "    '''\n",
    "    base_dir = '/sfs/gpfs/tardis/home/dcc7qe/LLM_project'\n",
    "    \n",
    "    model_name_dir = base_dir + f'/{model_name}_instruct'\n",
    "    pre_results_dir = model_name_dir + '/pretrain_results'\n",
    "    post_results_dir = model_name_dir + '/posttrain_results'\n",
    "    trained_model_dir = model_name_dir + '/trained_models'\n",
    "    best_model_results_dir = trained_model_dir + '/best_model'\n",
    "    \n",
    "    for new_dir in [model_name_dir, pre_results_dir, post_results_dir,\n",
    "                   trained_model_dir, best_model_results_dir]:\n",
    "        # create a directory\n",
    "        if not os.path.exists(new_dir):\n",
    "            os.mkdir(new_dir) \n",
    "\n",
    "    return(model_name_dir, pre_results_dir, post_results_dir,\n",
    "           trained_model_dir, best_model_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a88bc3-f0fa-452a-8f76-7aa89026683c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_dir, pre_results_dir, post_results_dir, trained_model_dir, best_model_results_dir = create_dirs('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c16fcc49-ed09-4583-ba84-3553df817075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_model_token(choice):\n",
    "    '''\n",
    "    Imports the model and tokenizer for the desired model\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(choice)\n",
    "    model = AutoModelForCausalLM.from_pretrained(choice)\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    print(model)\n",
    "    return(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5614ee6-1c7a-4aa6-9172-9c3517941da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50260, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### choose between llama3 and gpt2\n",
    "model_choice = \"vicgalle/gpt2-open-instruct-v1\" # used throughout the notebook\n",
    "#model_choice = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = import_model_token(model_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c999e9-e11a-48a5-8aa4-de63dd24e524",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Import and prepare the financial-phrasebank dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb383db-6736-4859-b56b-eb9d1a0cd76f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2a. Download and load into pandas. \n",
    "\n",
    "Create new column called \"Instruction\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61701758-d88c-45ec-81b8-c96ff51a1646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     According to Gran , the company has no plans t...      1\n",
       "1     Technopolis plans to develop in stages an area...      1\n",
       "2     The international electronic industry company ...      0\n",
       "3     With the new production plant the company woul...      2\n",
       "4     According to the company 's updated strategy f...      2\n",
       "...                                                 ...    ...\n",
       "4841  LONDON MarketWatch -- Share prices ended lower...      0\n",
       "4842  Rinkuskiai 's beer sales fell by 6.5 per cent ...      1\n",
       "4843  Operating profit fell to EUR 35.4 mn from EUR ...      0\n",
       "4844  Net sales of the Paper segment decreased to EU...      0\n",
       "4845  Sales in Finland decreased by 10.5 % in Januar...      0\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_phrase = load_dataset(\"warwickai/financial_phrasebank_mirror\")\n",
    "finance_phrase = pd.DataFrame(finance_phrase['train']) # this is the only split of the dataset on huggingface.\n",
    "finance_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4aeddaa-c5e4-405c-a3e8-d22302f9d8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3876, 2)\n",
      "(970, 2)\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "train, test = train_test_split(finance_phrase, test_size=0.2)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48960266-4c63-49c8-8cf9-7b5685e14f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['Instruction'] = train['sentence']\n",
    "test['Instruction'] = test['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae815c-b59d-43b8-a66a-7bf0d6fbc08a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2b. Create column called \"response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d3f3ad-2892-47be-ab5c-3aa9112ffd72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_538083/2595862958.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'negative' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train.loc[train['Response'] == old, ['Response']] = new\n",
      "/tmp/ipykernel_538083/2595862958.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'negative' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  test.loc[test['Response'] == old, ['Response']] = new\n"
     ]
    }
   ],
   "source": [
    "train['Response'] = train['label']\n",
    "for new, old in {'negative': 0, 'neutral': 1, 'positive': 2}.items():\n",
    "    train.loc[train['Response'] == old, ['Response']] = new \n",
    "    \n",
    "test['Response'] = test['label']\n",
    "for new, old in {'negative': 0, 'neutral': 1, 'positive': 2}.items():\n",
    "    test.loc[test['Response'] == old, ['Response']] = new "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdf401-8eb0-4350-b3f1-11e4aba38b38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2c. Set aside 9 examples for few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54eee7eb-cd13-48ae-beeb-26f7c4843f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 4)\n",
      "(3867, 4)\n"
     ]
    }
   ],
   "source": [
    "train_temp = pd.DataFrame()\n",
    "few_shot_fin = pd.DataFrame()\n",
    "for cat in list(train['Response'].unique()):\n",
    "    temp = train[train['Response'] == cat]\n",
    "    few_shot_fin = pd.concat([few_shot_fin, temp[-3:]])\n",
    "    train_temp = pd.concat([train_temp, temp[:-3]])\n",
    "print(few_shot_fin.shape)\n",
    "train = train_temp\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0294a7b1-1e17-4387-a9b2-91b9df610c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>Market data and analytics are derived from pri...</td>\n",
       "      <td>1</td>\n",
       "      <td>Market data and analytics are derived from pri...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>- Cash flow from operating activities before i...</td>\n",
       "      <td>1</td>\n",
       "      <td>- Cash flow from operating activities before i...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>The dollar bounced back after hitting another ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The dollar bounced back after hitting another ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>The major breweries increased their domestic b...</td>\n",
       "      <td>2</td>\n",
       "      <td>The major breweries increased their domestic b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>The Commission is to be applauded for applying...</td>\n",
       "      <td>2</td>\n",
       "      <td>The Commission is to be applauded for applying...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>In the third quarter , net sales increased by ...</td>\n",
       "      <td>2</td>\n",
       "      <td>In the third quarter , net sales increased by ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>Comparable operating profit decreased to EUR 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparable operating profit decreased to EUR 1...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>However , the growth margin slowed down due to...</td>\n",
       "      <td>0</td>\n",
       "      <td>However , the growth margin slowed down due to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>According to Arokarhu , some of the purchases ...</td>\n",
       "      <td>0</td>\n",
       "      <td>According to Arokarhu , some of the purchases ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  \\\n",
       "3023  Market data and analytics are derived from pri...      1   \n",
       "3466  - Cash flow from operating activities before i...      1   \n",
       "1983  The dollar bounced back after hitting another ...      1   \n",
       "132   The major breweries increased their domestic b...      2   \n",
       "1915  The Commission is to be applauded for applying...      2   \n",
       "220   In the third quarter , net sales increased by ...      2   \n",
       "4664  Comparable operating profit decreased to EUR 1...      0   \n",
       "544   However , the growth margin slowed down due to...      0   \n",
       "1150  According to Arokarhu , some of the purchases ...      0   \n",
       "\n",
       "                                            Instruction  Response  \n",
       "3023  Market data and analytics are derived from pri...   neutral  \n",
       "3466  - Cash flow from operating activities before i...   neutral  \n",
       "1983  The dollar bounced back after hitting another ...   neutral  \n",
       "132   The major breweries increased their domestic b...  positive  \n",
       "1915  The Commission is to be applauded for applying...  positive  \n",
       "220   In the third quarter , net sales increased by ...  positive  \n",
       "4664  Comparable operating profit decreased to EUR 1...  negative  \n",
       "544   However , the growth margin slowed down due to...  negative  \n",
       "1150  According to Arokarhu , some of the purchases ...  negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_fin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef792f1-45ac-4dc8-bc26-ceb81885c24e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2d. Show the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3d860f-46a8-4692-92af-dd0b746283e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>Handelsbanken ranked before Local Cooperative ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Handelsbanken ranked before Local Cooperative ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>Brazilian non-profit interbank Camara Interban...</td>\n",
       "      <td>2</td>\n",
       "      <td>Brazilian non-profit interbank Camara Interban...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>CEO of the company Tarmo Noop said the growth ...</td>\n",
       "      <td>2</td>\n",
       "      <td>CEO of the company Tarmo Noop said the growth ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>The company has established a 3G base station ...</td>\n",
       "      <td>2</td>\n",
       "      <td>The company has established a 3G base station ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>`` Indo-Russia trade can cross the targeted 10...</td>\n",
       "      <td>2</td>\n",
       "      <td>`` Indo-Russia trade can cross the targeted 10...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  \\\n",
       "1531  Handelsbanken ranked before Local Cooperative ...      2   \n",
       "2260  Brazilian non-profit interbank Camara Interban...      2   \n",
       "2272  CEO of the company Tarmo Noop said the growth ...      2   \n",
       "898   The company has established a 3G base station ...      2   \n",
       "1951  `` Indo-Russia trade can cross the targeted 10...      2   \n",
       "\n",
       "                                            Instruction  Response  \n",
       "1531  Handelsbanken ranked before Local Cooperative ...  positive  \n",
       "2260  Brazilian non-profit interbank Camara Interban...  positive  \n",
       "2272  CEO of the company Tarmo Noop said the growth ...  positive  \n",
       "898   The company has established a 3G base station ...  positive  \n",
       "1951  `` Indo-Russia trade can cross the targeted 10...  positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0182d5-29f9-45ee-becb-35a5de4f746f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2e. Concatenate two different instruction-response pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0982b9-be40-433c-8432-ed0bd1cf5746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handelsbanken ranked before Local Cooperative Banks and Aktia in customer loyalty this time too , however .\n",
      "\n",
      "Brazilian non-profit interbank Camara Interbancaria de Pagamentos CIP has acquired solutions from US business integration solutions provider Sterling Commerce , the latter company said in a statement . \n",
      "\n",
      "positive\n",
      "\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(list(train['Instruction'])[0] + '\\n\\n' + list(train['Instruction'])[1], \"\\n\")\n",
    "\n",
    "print(list(train['Response'])[0] + '\\n\\n' + list(train['Response'])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cfee5-a7ec-4a09-8337-b8f6c9a0f807",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2f. Add instruct column to the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b2a11d-5ca3-49bf-b51e-d03224815109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3867, 4)\n",
      "(970, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36cf4501-bae1-4dd6-a955-3fdeab95d7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_copy = train.copy()\n",
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55d9bba4-78a9-4fe8-9e6b-e036ebb9fcfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[['Instruction', 'Response']]\n",
    "train.columns = ['question', 'answer']\n",
    "test = test[['Instruction', 'Response']]\n",
    "test.columns = ['question', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49b2bb0-6a17-47f6-a0be-209ce9c43dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_538083/3729892796.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['instruct'] = instruct_list\n"
     ]
    }
   ],
   "source": [
    "# formats into the proper instruction/response format\n",
    "options = 'positive, neutral, negative'\n",
    "\n",
    "#### training data\n",
    "question_list = list(train['question'])\n",
    "answer_list = list(train['answer'])\n",
    "instruct_list = []\n",
    "question_formatted = []\n",
    "\n",
    "for ind in range(len(question_list)):\n",
    "    \n",
    "    q = question_list[ind]\n",
    "    #instruction = f'{q} Choose one of the following words: {options}'\n",
    "    instruction = f'{q}'\n",
    "    question_formatted.append(instruction)\n",
    "    \n",
    "    response = answer_list[ind]\n",
    "        \n",
    "    instruct = f\"Instruct: {instruction}\\n\\nResponse: {response}\"\n",
    "    instruct_list.append(instruct)\n",
    "\n",
    "# without reducing to only the needed columns, the tensors cannot be processed. \n",
    "train['instruct'] = instruct_list\n",
    "train['question_formatted'] = question_formatted\n",
    "train = train[['question_formatted', 'answer', 'instruct']]\n",
    "train.columns = ['question', 'answer', 'instruct']\n",
    "\n",
    "#### test data\n",
    "question_list = list(test['question'])\n",
    "answer_list = list(test['answer'])\n",
    "instruct_list = []\n",
    "question_formatted = []\n",
    "\n",
    "for ind in range(len(question_list)):\n",
    "    \n",
    "    q = question_list[ind]\n",
    "    instruction = f'{q} Choose one of the following words: {options}'\n",
    "    question_formatted.append(instruction)\n",
    "    \n",
    "    response = answer_list[ind]\n",
    "        \n",
    "    instruct = f\"Instruct: {instruction}\\n\\nResponse: {response}\"\n",
    "    instruct_list.append(instruct)\n",
    "\n",
    "# without reducing to only the needed columns, the tensors cannot be processed. \n",
    "test['instruct'] = instruct_list\n",
    "test['question_formatted'] = question_formatted\n",
    "test = test[['question_formatted', 'answer', 'instruct']]\n",
    "test.columns = ['question', 'answer', 'instruct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64a76448-6711-4baa-84b8-05691f22d256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>`` BG Crane has been a strong partner for Hiab...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Instruct: `` BG Crane has been a strong partne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question   answer  \\\n",
       "1004  `` BG Crane has been a strong partner for Hiab...  neutral   \n",
       "\n",
       "                                               instruct  \n",
       "1004  Instruct: `` BG Crane has been a strong partne...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73392651-4495-4692-88e0-715e9e2b29cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>Okhta Center area is expected to have about 70...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Instruct: Okhta Center area is expected to hav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question   answer  \\\n",
       "3063  Okhta Center area is expected to have about 70...  neutral   \n",
       "\n",
       "                                               instruct  \n",
       "3063  Instruct: Okhta Center area is expected to hav...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41034d6c-2168-432a-9f17-e8ca58f658ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5809b9df-87c9-4b29-afc5-25af06bdb5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_fin_sentiment_instruction = \"\"\"You are a financial expert that analyzes financial news reports. Each report contains numbers and \n",
    "                                    qualitative descriptions of a subject's financial performance. \n",
    "                                    Your job is to classify the sentiment of each report as one of the predefined categories.\n",
    "                                    Sentiment categories: ['positive', 'neutral', 'negative']. Reply with one word only.\n",
    "                                    Following are a few examples of similar text and the correct category. \n",
    "                                    Provide the sentiment category for the last body of text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22233bdd-05c1-407d-849e-67ebfb7c1930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(question = '', instruction = False,\n",
    "               few_shot_instruction = [], few_shot_response = []):\n",
    "    '''\n",
    "    Receives the few_shot example questions and responses. \n",
    "    Receives the prompt question and its label, if testing.\n",
    "    '''\n",
    "    \n",
    "    if (instruction == False):\n",
    "        prompt_instruction = standard_fin_sentiment_instruction\n",
    "    else: \n",
    "        prompt_instruction = instruction # different instruction for multifin benchmark vs inference on the prompt-tuned model\n",
    "        \n",
    "    \n",
    "    for ind in range(len(few_shot_instruction)):\n",
    "        prompt_instruction = prompt_instruction + \"\\nText: {\" + few_shot_instruction[ind] + \"}\\nAnswer: {\" + few_shot_response[ind] + \"}\"\n",
    "    \n",
    "    prompt = prompt_instruction + \"\\nText: {\" + question + \"}\\nAnswer:\"\n",
    "    return(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d59e6b-4ccc-41b5-be6b-f7d93aeb65d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pipe():\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    device_map=\"auto\")\n",
    "    return(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce110be1-a771-4593-9596-d64db177ff09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model(model_pipe, \n",
    "                question = '', instruction = False, \n",
    "                few_shot_instruction = [], few_shot_response = [],\n",
    "                formatted_response = True\n",
    "                ): # add parameters for testing vs inference?\n",
    "    \n",
    "    prompt = get_prompt(question, instruction, few_shot_instruction, few_shot_response)\n",
    "    \n",
    "    # queries the model via pipe\n",
    "    sequences = model_pipe(prompt,\n",
    "                           max_new_tokens=3, #### **important parameter - needs to == 3 for GPT2 instruct, for a concise response\n",
    "                           do_sample=True,\n",
    "                           top_k=10)\n",
    "    \n",
    "    if (formatted_response == True):\n",
    "        formatted_response = format_response(sequences)\n",
    "    else: formatted_response = ''\n",
    "    \n",
    "    return(sequences, formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1620fed6-b828-4eda-acd5-13e7c753f4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_response(output):\n",
    "    ##### I need to modify this so that it finds the last instance of the 'answer' and captures that.\n",
    "    \n",
    "    text_raw = output[0]['generated_text'].split('\\n')\n",
    "    text_raw = [s for s in text_raw if (s.startswith('Text') or s.startswith('Answer'))]\n",
    "    full_text = text_raw[-2:]\n",
    "    \n",
    "    answer = [s for s in full_text if (s.startswith('Answer'))][0]#.split(':')\n",
    "    #answer = answer[answer.find('{'):]\n",
    "    answer = answer[answer.find(':') + 1:]\n",
    "    ############\n",
    "    '''\n",
    "    if (',' in answer):\n",
    "        answer = answer.split(',')[:1][0]\n",
    "    if (' ' in answer):\n",
    "        answer = answer.split(' ')\n",
    "    \n",
    "    answer = [s for s in answer if (s != '')]\n",
    "    if (len(answer) > 2): answer = answer[:1][0]\n",
    "    else: answer = answer[0]\n",
    "    answer = re.sub('[^a-zA-Z]', '', answer)\n",
    "    #############\n",
    "    '''\n",
    "    answer = re.sub('[^a-zA-Z]', '', answer)\n",
    "    answer = re.sub(' ', '', answer)\n",
    "    \n",
    "    \n",
    "    formatted_response = {'full_text': full_text,\n",
    "                          'model_answer': answer}\n",
    "    \n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d494402-2c26-42c4-9988-15aff56f26d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Benchmarks: evaluate before training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72e43b-f04c-46c8-8ce1-b509f3ec7ecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare custom benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37479b-02d0-4bcf-ba16-1503871f6c2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Benchmark data - multifin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a2c4425-03c7-42a5-963d-f778e4ff9ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>text</th>\n",
       "      <th>choices</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multifineng0</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Deal summary Helen Oy / Infratek Finland Oy</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multifineng1</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Deals</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multifineng2</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>PwC named a Major Player in the IDC MarketScap...</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multifineng3</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Tax &amp; Accounting</td>\n",
       "      <td>Proposal for digital services tax</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multifineng4</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Business &amp; Management</td>\n",
       "      <td>Social Impact Lab</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              query  \\\n",
       "0  multifineng0  In this task, you're working with English head...   \n",
       "1  multifineng1  In this task, you're working with English head...   \n",
       "2  multifineng2  In this task, you're working with English head...   \n",
       "3  multifineng3  In this task, you're working with English head...   \n",
       "4  multifineng4  In this task, you're working with English head...   \n",
       "\n",
       "                  answer                                               text  \\\n",
       "0                Finance        Deal summary Helen Oy / Infratek Finland Oy   \n",
       "1                Finance                                              Deals   \n",
       "2             Technology  PwC named a Major Player in the IDC MarketScap...   \n",
       "3       Tax & Accounting                 Proposal for digital services tax    \n",
       "4  Business & Management                                  Social Impact Lab   \n",
       "\n",
       "                                             choices  gold  \n",
       "0  [Finance, Technology, Tax & Accounting, Busine...     0  \n",
       "1  [Finance, Technology, Tax & Accounting, Busine...     0  \n",
       "2  [Finance, Technology, Tax & Accounting, Busine...     1  \n",
       "3  [Finance, Technology, Tax & Accounting, Busine...     2  \n",
       "4  [Finance, Technology, Tax & Accounting, Busine...     3  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multifin = load_dataset(\"ChanceFocus/flare-multifin-en\")\n",
    "multifin = pd.DataFrame(multifin['test']) # this is the only split of the dataset on huggingface.\n",
    "multifin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ba5224e-a2fe-438f-9201-8b80ef5d2e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Finance', 'Technology', 'Tax & Accounting',\n",
       "       'Business & Management', 'Government & Controls', 'Industry'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(multifin.shape)\n",
    "multifin['answer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65841397-2d7d-4845-96ed-8465e5dc0309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 6)\n",
      "(534, 6)\n"
     ]
    }
   ],
   "source": [
    "few_shot_df = pd.DataFrame()\n",
    "bench_df = pd.DataFrame()\n",
    "for cat in list(multifin['answer'].unique()):\n",
    "    temp = multifin[multifin['answer'] == cat]\n",
    "    few_shot_df = pd.concat([few_shot_df, temp[-2:]])\n",
    "    bench_df = pd.concat([bench_df, temp[:-2]])\n",
    "print(few_shot_df.shape)\n",
    "print(bench_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09219b2d-4619-4910-b252-a06aab709bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>text</th>\n",
       "      <th>choices</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>multifineng539</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Finance</td>\n",
       "      <td>PwC participated in the opening of the market ...</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>multifineng545</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Removal of the annual limit of the pension and...</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>multifineng520</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Technology has reduced the tax compliance burd...</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>multifineng544</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Information Security Strategy</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>multifineng542</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Tax &amp; Accounting</td>\n",
       "      <td>Tax reporting &amp; strategy</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>multifineng543</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Tax &amp; Accounting</td>\n",
       "      <td>Taxes in Iceland</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>multifineng536</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Business &amp; Management</td>\n",
       "      <td>The evolving nature of the green agenda</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>multifineng541</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Business &amp; Management</td>\n",
       "      <td>Generational transfer</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>multifineng511</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Government &amp; Controls</td>\n",
       "      <td>SOX and internal control</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>multifineng533</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Government &amp; Controls</td>\n",
       "      <td>Government and Public Sector</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>multifineng538</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Industry</td>\n",
       "      <td>PwC announces the short list of nominees for t...</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>multifineng540</td>\n",
       "      <td>In this task, you're working with English head...</td>\n",
       "      <td>Industry</td>\n",
       "      <td>Direct to Consumer</td>\n",
       "      <td>[Finance, Technology, Tax &amp; Accounting, Busine...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                              query  \\\n",
       "539  multifineng539  In this task, you're working with English head...   \n",
       "545  multifineng545  In this task, you're working with English head...   \n",
       "520  multifineng520  In this task, you're working with English head...   \n",
       "544  multifineng544  In this task, you're working with English head...   \n",
       "542  multifineng542  In this task, you're working with English head...   \n",
       "543  multifineng543  In this task, you're working with English head...   \n",
       "536  multifineng536  In this task, you're working with English head...   \n",
       "541  multifineng541  In this task, you're working with English head...   \n",
       "511  multifineng511  In this task, you're working with English head...   \n",
       "533  multifineng533  In this task, you're working with English head...   \n",
       "538  multifineng538  In this task, you're working with English head...   \n",
       "540  multifineng540  In this task, you're working with English head...   \n",
       "\n",
       "                    answer                                               text  \\\n",
       "539                Finance  PwC participated in the opening of the market ...   \n",
       "545                Finance  Removal of the annual limit of the pension and...   \n",
       "520             Technology  Technology has reduced the tax compliance burd...   \n",
       "544             Technology                      Information Security Strategy   \n",
       "542       Tax & Accounting                           Tax reporting & strategy   \n",
       "543       Tax & Accounting                                   Taxes in Iceland   \n",
       "536  Business & Management            The evolving nature of the green agenda   \n",
       "541  Business & Management                              Generational transfer   \n",
       "511  Government & Controls                           SOX and internal control   \n",
       "533  Government & Controls                       Government and Public Sector   \n",
       "538               Industry  PwC announces the short list of nominees for t...   \n",
       "540               Industry                                 Direct to Consumer   \n",
       "\n",
       "                                               choices  gold  \n",
       "539  [Finance, Technology, Tax & Accounting, Busine...     0  \n",
       "545  [Finance, Technology, Tax & Accounting, Busine...     0  \n",
       "520  [Finance, Technology, Tax & Accounting, Busine...     1  \n",
       "544  [Finance, Technology, Tax & Accounting, Busine...     1  \n",
       "542  [Finance, Technology, Tax & Accounting, Busine...     2  \n",
       "543  [Finance, Technology, Tax & Accounting, Busine...     2  \n",
       "536  [Finance, Technology, Tax & Accounting, Busine...     3  \n",
       "541  [Finance, Technology, Tax & Accounting, Busine...     3  \n",
       "511  [Finance, Technology, Tax & Accounting, Busine...     4  \n",
       "533  [Finance, Technology, Tax & Accounting, Busine...     4  \n",
       "538  [Finance, Technology, Tax & Accounting, Busine...     5  \n",
       "540  [Finance, Technology, Tax & Accounting, Busine...     5  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2aded39-990b-4691-af7f-b695b52999d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shot_instruction = list(few_shot_df['text'])\n",
    "few_shot_response = list(few_shot_df['answer'])\n",
    "bench_questions = list(bench_df['text']) # for running benchmark\n",
    "bench_labels = list(bench_df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed0741-cd6c-460a-b10e-b11857b40bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cbd77762-6985-4941-9ab3-b2599824ba63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multifin_prompt_instruction = \"\"\"In this task, you're working with English headlines. \n",
    "                        This dataset is made up of real-world article headlines from a large accounting firm's websites. \n",
    "                        Your objective is to categorize each headline as one of the predefined categories.\n",
    "                        Categories: ['Finance', 'Technology', 'Tax & Accounting', 'Business & Management', \n",
    "                        'Government & Controls', 'Industry']. Reply with one word only. \n",
    "                        Following are a few examples of similar text and the correct category.\n",
    "                        Provide the answer for the last body of text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "587a5272-7202-4309-b883-f9e127df0a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_prompt(question='', instruction=False, few_shot_instruction=[], few_shot_response=[])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45901c4f-c18d-4a3a-889a-d486d23d563b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_pipe()>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03436d21-e78a-4747-a48e-e333d53b81a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.query_model(model_pipe, question='', instruction=False, few_shot_instruction=[], few_shot_response=[], formatted_response=True)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f1938dc-3c21-40ab-b896-697fa72fa38c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.format_response(output)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2faac419-9c62-4320-8426-dbc939944959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "at this point I am not renaming the function, because I have ran it so many times.\n",
    "However, it should really just be called \"run_custom_benchmark\", since it can be ran on multifin or fin_phrasebank datasets.\n",
    "'''\n",
    "def run_multifin_benchmark(few_shot_instruction, few_shot_response,\n",
    "                          bench_questions, bench_labels, instruction = False):\n",
    "    \n",
    "    ###### for multifin, but could really apply to any dataset\n",
    "    \n",
    "    # set pipeline\n",
    "    model_pipe = get_pipe()\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    # set a results variable to receive for each question: full sequences, formatted_response, label\n",
    "    results_multifin = {'samples': []}\n",
    "\n",
    "    #instruction = multifin_prompt_instruction\n",
    "    \n",
    "    print(f\"{len(bench_questions)} questions to answer\\nNumber completed: \")\n",
    "    i = 1\n",
    "    # need to loop through all the benchmark test data\n",
    "    for ind in range(len(bench_questions)):\n",
    "    \n",
    "        question, label = bench_questions[ind], bench_labels[ind]\n",
    "    \n",
    "        # within the loop, query the model and receive the model answer\n",
    "        raw_output, formatted_response = query_model(model_pipe, question, instruction,\n",
    "                                                     few_shot_instruction, few_shot_response,\n",
    "                                                     formatted_response = True)\n",
    "            \n",
    "        #### check the answer against the label to determine accuracy\n",
    "        model_answer = formatted_response['model_answer']\n",
    "            \n",
    "        # question's accuracy - exact\n",
    "        if (model_answer.upper() == label.upper()):\n",
    "            question_acc = 1.0\n",
    "        else: question_acc = 0.0\n",
    "        \n",
    "        # question's relative accuracy - takes into account proximity to the correct answer\n",
    "        if (instruction == False): # if false, then we are using the standard prompt and 3 categories: negative, neutral, positive\n",
    "            \n",
    "            ### there's probably a more concise algorithm, but this will work.\n",
    "            \n",
    "            if (model_answer.upper() == 'NEGATIVE'):\n",
    "                \n",
    "                if (question_acc == 1.0): question_acc_prox = 1.0\n",
    "                elif (label.upper() == 'NEUTRAL'): question_acc_prox = 0.5\n",
    "                else: question_acc_prox = 0.0 # == positive\n",
    "                \n",
    "            elif (model_answer.upper() == 'NEUTRAL'):\n",
    "                \n",
    "                if (question_acc == 1.0): question_acc_prox = 1.0\n",
    "                elif (label.upper() == 'NEGATIVE'): question_acc_prox = 0.25 ### not sure if this should be weighted as 0.25? too easy and rewards for 0.5\n",
    "                else: question_acc_prox = 0.25 # correct answer is positive\n",
    "                \n",
    "            elif (model_answer.upper() == 'POSITIVE'):\n",
    "                \n",
    "                if (question_acc == 1.0): question_acc_prox = 1.0\n",
    "                elif (label.upper() == 'NEUTRAL'): question_acc_prox = 0.5\n",
    "                else: question_acc_prox = 0.0 # correct answer is negative\n",
    "                \n",
    "                \n",
    "            else: question_acc_prox = 0.0 # any other answer is not the right format or one of the three choices\n",
    "        else: question_acc_prox = 0.0\n",
    "        \n",
    "        # add an entry to the results['samples'] for this question - with each accuracy score\n",
    "        question_results = {'raw_output': raw_output,\n",
    "                            'formatted_response': formatted_response,\n",
    "                            'model_answer': model_answer,\n",
    "                            'correct_answer': label,\n",
    "                            'accuracy_question': question_acc,\n",
    "                            'accuracy_proximity_question': question_acc_prox\n",
    "                           }\n",
    "        \n",
    "        results_multifin['samples'].append(question_results)\n",
    "        \n",
    "        if (i % 50 == 0): print(i, end = ' ')\n",
    "        i += 1\n",
    "                \n",
    "            \n",
    "    ###### EXACT accuracy\n",
    "    # determine overall accuracy for the benchmark\n",
    "    num_samples = len(results_multifin['samples'])\n",
    "    acc_list = [res['accuracy_question'] for res in results_multifin['samples']]\n",
    "    acc_aggregate = 0\n",
    "    for acc in acc_list:\n",
    "        acc_aggregate += acc\n",
    "    model_accuracy = acc_aggregate / num_samples\n",
    "    \n",
    "    # save as separate key in the results variable\n",
    "    results_multifin['benchmark_accuracy_exact'] = model_accuracy\n",
    "    \n",
    "    ###### PROXIMITY accuracy\n",
    "    if (instruction == False): # if false, then we are using the standard prompt and 3 categories: negative, neutral, positive\n",
    "         # determine overall accuracy for the benchmark\n",
    "        num_samples = len(results_multifin['samples'])\n",
    "        acc_list = [res['accuracy_proximity_question'] for res in results_multifin['samples']]\n",
    "        acc_aggregate = 0\n",
    "        for acc in acc_list:\n",
    "            acc_aggregate += acc\n",
    "        model_accuracy_prox = acc_aggregate / num_samples \n",
    "    else: model_accuracy_prox = None\n",
    "        \n",
    "    # add the accuracy_proximity score\n",
    "    results_multifin['benchmark_accuracy_proximate'] = model_accuracy_prox\n",
    "    \n",
    "    return(results_multifin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d53fd3-ec24-462a-b59b-ae99dceb66f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### (old) testing functions with small samples -> runs well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2ca4cc7-0608-4e1c-9dc8-904e657e72ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>According to CEO Matti Perkonoja of the parent...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instruct: According to CEO Matti Perkonoja of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>Earlier today , Geberit 's Finnish rival Upono...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instruct: Earlier today , Geberit 's Finnish r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>Possible personnel reductions concern approxim...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instruct: Possible personnel reductions concer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>Operating profit decreased to EUR 11.2 mn from...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instruct: Operating profit decreased to EUR 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>A survey conducted by Taloustutkimus for Sampo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instruct: A survey conducted by Taloustutkimus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question    answer  \\\n",
       "4008  According to CEO Matti Perkonoja of the parent...  negative   \n",
       "4591  Earlier today , Geberit 's Finnish rival Upono...  negative   \n",
       "4126  Possible personnel reductions concern approxim...  negative   \n",
       "4747  Operating profit decreased to EUR 11.2 mn from...  negative   \n",
       "4440  A survey conducted by Taloustutkimus for Sampo...  negative   \n",
       "\n",
       "                                               instruct  \n",
       "4008  Instruct: According to CEO Matti Perkonoja of ...  \n",
       "4591  Instruct: Earlier today , Geberit 's Finnish r...  \n",
       "4126  Instruct: Possible personnel reductions concer...  \n",
       "4747  Instruct: Operating profit decreased to EUR 11...  \n",
       "4440  Instruct: A survey conducted by Taloustutkimus...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "90c674de-cd6c-417f-8a08-1eb887a3a17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa84915d690>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set pipeline\n",
    "model_pipe = get_pipe()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f396433c-6dc1-4b57-b84a-a7ed7a96e14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shot_fin_instruction = list(few_shot_fin['Instruction'])\n",
    "few_shot_fin_response = list(few_shot_fin['Response'])\n",
    "examples_instruction = list(test['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "02af4466-38bb-4836-bad9-d547015c1712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): PeftModelForCausalLM(\n",
       "    (base_model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50260, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Conv1D(nf=2304, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       "    )\n",
       "    (prompt_encoder): ModuleDict(\n",
       "      (default): PromptEmbedding(\n",
       "        (embedding): Embedding(10, 768)\n",
       "      )\n",
       "    )\n",
       "    (word_embeddings): Embedding(50260, 768)\n",
       "  )\n",
       "  (word_embeddings): Embedding(50260, 768)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "65a6d35e-6d78-4de2-bd4f-3401e2683b59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m raw_output, formatted_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples_instruction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfew_shot_fin_instruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfew_shot_fin_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mformatted_response\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# pipe_ was declared earlier. Global var\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 10\u001b[0m, in \u001b[0;36mquery_model\u001b[0;34m(model_pipe, question, instruction, few_shot_instruction, few_shot_response, formatted_response)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m get_prompt(question, instruction, few_shot_instruction, few_shot_response)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# queries the model via pipe\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#### **important parameter - needs to == 3 for GPT2 instruct, for a concise response\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (formatted_response \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     formatted_response \u001b[38;5;241m=\u001b[39m format_response(sequences)\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/pipelines/base.py:1368\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1362\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         )\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/pipelines/base.py:1375\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1374\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1375\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1274\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1275\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    383\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 385\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    388\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/peft/peft_model.py:1840\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1838\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1840\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/peft/peft_model.py:1840\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1838\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1840\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/generation/utils.py:3204\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3199\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3201\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3202\u001b[0m ):\n\u001b[1;32m   3203\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 3204\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3206\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[1;32m   3207\u001b[0m     model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/peft/peft_model.py:1882\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.prepare_inputs_for_generation\u001b[0;34m(self, task_ids, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1882\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m], peft_config\u001b[38;5;241m.\u001b[39mnum_virtual_tokens\n\u001b[1;32m   1883\u001b[0m     prefix_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(size)\u001b[38;5;241m.\u001b[39mto(model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1884\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m   1885\u001b[0m         (prefix_attention_mask, model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1886\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "raw_output, formatted_response = query_model(model_pipe, examples_instruction[0], False, few_shot_fin_instruction, few_shot_fin_response,\n",
    "                                             formatted_response = True) # pipe_ was declared earlier. Global var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e76d884-386c-4be6-9f98-b1b2302e5a1e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Instruction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Instruction'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# the question to ask \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m examples_instruction_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInstruction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)[\u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m      3\u001b[0m examples_response_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m      4\u001b[0m few_shot_instruction_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstruction\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m9\u001b[39m:\u001b[38;5;241m12\u001b[39m]\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Instruction'"
     ]
    }
   ],
   "source": [
    "# the question to ask \n",
    "examples_instruction_test = list(train['question'])[6:8]\n",
    "examples_response_test = list(train['answer'])[6:8]\n",
    "few_shot_instruction_test = list(train['Instruction'])[9:12]\n",
    "few_shot_response_test = list(train['Response'])[9:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7dbf66b0-8876-406e-bc50-4f04ed073efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "raw_output, formatted_response = query_model(model_pipe, examples_instruction[0], False, few_shot_fin_instruction, few_shot_fin_response,\n",
    "                                             formatted_response = True) # pipe_ was declared earlier. Global var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ae49d84-0be2-4738-b034-2be16c227536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial expert that analyzes financial news reports. Each report contains numbers and \n",
      "                            qualitative descriptions of a subject's financial performance. \n",
      "                            Your job is to classify the sentiment of each report as one of the predefined categories.\n",
      "                            Categories: ['positive', 'neutral', 'negative']. Reply with one word only.\n",
      "                            Following are a few examples of similar text and the correct category. \n",
      "                            Provide the answer for the last body of text.\n",
      "Text: {Operating profit in the fourth quarter went down to EUR3m from EUR4 .2 m for the corresponding period of 2009 as it included costs of growth projects .}\n",
      "Answer: {negative}\n",
      "Text: {( ADPnews ) - Feb 3 , 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR 46 million ( USD 64.5 m ) in the fourth quarter of 2009 from a}\n",
      "Answer: {negative}\n",
      "Text: {The company confirmed its estimate for lower revenue for the whole 2009 than the year-ago EUR93 .9 m as given in the interim report on 5 August 2009 .}\n",
      "Answer: {negative}\n",
      "Text: {Sami Sepp+Ã±nen , CEO for Elisa Eesti , says the novel network allows fast mobile broadband for sparsely populated areas .}\n",
      "Answer: {neutral}\n",
      "Text: {A total of 185 Wonderware Certified SIs are available to integrate and support Wonderware products such as InTouch -Â« HMI software , IndustrialSQL Server historian , Wonderware Information Server , DT Analyst software or QI Analyst SPC software .}\n",
      "Answer: {neutral}\n",
      "Text: {The Finland-based company says it will move into an existing 260,000-square-foot facility in September .}\n",
      "Answer: {neutral}\n",
      "Text: {Finnish metal industry solutions supplier Outotec Oyj net profit rose to 50.4 mln euro ( $ 72.5 mln ) for the first nine months of 2007 from 20.1 mln euro ( $ 28.9 mln ) for the same period of 2006 .}\n",
      "Answer: {positive}\n",
      "Text: {Nordea Bank ( STO : NDA ) and Sampo Bank have helped Finnish real estate investment company Sponda ( HEL : SDA1V ) place a EUR100m ( USD125 .4 m ) domestic bond , the company said Friday .}\n",
      "Answer: {positive}\n",
      "Text: {In contrast , the company 's net loss for the third quarter of 2009 contracted to EUR 76 million from EUR 256 million for the corresponding period a year ago .}\n",
      "Answer: {positive}\n",
      "Text: {TeliaSonera said about $ 100 million will be invested in the next year in the region to bring mobile coverage to about 90 % of Nepal s population . Choose one of the following words: positive, neutral, negative}\n",
      "Answer: {neutral}\n"
     ]
    }
   ],
   "source": [
    "print(raw_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "02fa2a88-a1e8-4d9d-88db-c285558fd630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_text': ['Text: {TeliaSonera said about $ 100 million will be invested in the next year in the region to bring mobile coverage to about 90 % of Nepal s population . Choose one of the following words: positive, neutral, negative}',\n",
       "  'Answer: {neutral}'],\n",
       " 'model_answer': 'neutral'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b8933c2-0b25-4e20-9eb1-5834e85b5da4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "results_multifin = run_multifin_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bcb50e62-61b8-45ac-99a8-a3fdbf120c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samples': [{'raw_output': [{'generated_text': \"You are a financial expert that analyzes financial news reports. Each report contains numbers and \\n                        qualitative descriptions of a subject's financial performance. \\n                        Your job is to classify the sentiment of each report as one of the predefined categories.\\n                        Categories: ['positive', 'neutral', 'negative']. Reply with one word only.\\n                        Following are a few examples of similar text and the correct category. \\n                        Provide the answer for the last body of text.\\nText: {All other charges were dismissed .}\\nAnswer: {neutral}\\nText: {Automation makes it possible to conduct several tests simultaneously .}\\nAnswer: {neutral}\\nText: {`` However , the rapidly increasing costs and the strengthening of the euro were challenging for the whole industry , and they impacted on our results . ''}\\nAnswer: {negative}\\nText: {Ahlstrom , headquartered in Helsinki , Finland , is a global leader in the development , manufacture and marketing of high performance fibre-based materials .}\\nAnswer: {neutral}\"}],\n",
       "   'formatted_response': {'full_text': ['Text: {Ahlstrom , headquartered in Helsinki , Finland , is a global leader in the development , manufacture and marketing of high performance fibre-based materials .}',\n",
       "     'Answer: {neutral}'],\n",
       "    'model_answer': 'neutral'},\n",
       "   'model_answer': 'neutral',\n",
       "   'correct_answer': 'neutral',\n",
       "   'accuracy_question': 1.0},\n",
       "  {'raw_output': [{'generated_text': \"You are a financial expert that analyzes financial news reports. Each report contains numbers and \\n                        qualitative descriptions of a subject's financial performance. \\n                        Your job is to classify the sentiment of each report as one of the predefined categories.\\n                        Categories: ['positive', 'neutral', 'negative']. Reply with one word only.\\n                        Following are a few examples of similar text and the correct category. \\n                        Provide the answer for the last body of text.\\nText: {All other charges were dismissed .}\\nAnswer: {neutral}\\nText: {Automation makes it possible to conduct several tests simultaneously .}\\nAnswer: {neutral}\\nText: {`` However , the rapidly increasing costs and the strengthening of the euro were challenging for the whole industry , and they impacted on our results . ''}\\nAnswer: {negative}\\nText: {To achieve synergy targets related to the acquisition , Vaisala is consolidating its manufacturing , depot and data services , and finance activities currently located in Uniontown , Pennsylvania , St. Louis , Missouri and Durham , North Carolina .}\\nAnswer: {neutral}\"}],\n",
       "   'formatted_response': {'full_text': ['Text: {To achieve synergy targets related to the acquisition , Vaisala is consolidating its manufacturing , depot and data services , and finance activities currently located in Uniontown , Pennsylvania , St. Louis , Missouri and Durham , North Carolina .}',\n",
       "     'Answer: {neutral}'],\n",
       "    'model_answer': 'neutral'},\n",
       "   'model_answer': 'neutral',\n",
       "   'correct_answer': 'neutral',\n",
       "   'accuracy_question': 1.0}],\n",
       " 'benchmark_accuracy': 1.0}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_multifin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c130f-04b1-4f7b-b391-1b6183a99e89",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run all benchmarks\n",
    "\n",
    "- arc_easy - general\n",
    "- bbh_zeroshot_causal_judgement - general\n",
    "- custom multifin - specialized\n",
    "- custom fin phrasebank test data - specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c68e00-546b-4541-8991-1b5dea4716dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up lm_eval task manager and implement\n",
    "task_manager = lm_eval.tasks.TaskManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b341c-02ea-4f14-a0cc-9dc20f9e4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(pre_results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7b61b-527c-467c-b4ae-ff7e6d6bac08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 1. arc_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "235b297f-7fb6-4b6b-88c2-0ba1b1f75243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "Running generate_until requests:   1%|          | 2/187 [46:21<71:28:25, 1390.84s/it]\n",
      "Running loglikelihood requests:   2%|â–         | 165/9501 [45:47<43:11:01, 16.65s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:01<00:00, 2174.65it/s]\n",
      "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9501/9501 [12:47<00:00, 12.38it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /sfs/gpfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "results_arc = lm_eval.simple_evaluate(\n",
    "    model = \"hf\", \n",
    "    model_args = {\"pretrained\": model, \n",
    "                  \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer},\n",
    "    tasks = ['arc_easy'\n",
    "    ],\n",
    "    task_manager = task_manager,\n",
    "    log_samples = True, # set to False\n",
    "    batch_size = 30,\n",
    "    #limit = 15, # using the full dataset\n",
    "    random_seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5312b48b-3089-42d5-9ff9-2416921245a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del results_arc['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5660ca6-d1aa-4f76-baae-b4649c376e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(pre_results_dir)\n",
    "with open('results_arc_easy_json', 'w') as f:\n",
    "    json.dump(results_arc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0f49f-cf95-492a-89a9-69ec40b424cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 2. bbh_causal_judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edd0b121-05d1-40f2-a3ed-7c2920d18df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "[Task: bbh_zeroshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "[Task: bbh_zeroshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [00:00<00:00, 3069.01it/s]\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [10:25<00:00,  3.35s/it] \n",
      "fatal: not a git repository (or any parent up to mount point /sfs/gpfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "results_bbh = lm_eval.simple_evaluate(\n",
    "    model = \"hf\", \n",
    "    model_args = {\"pretrained\": model, \n",
    "                  \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer},\n",
    "    tasks = ['bbh_zeroshot_causal_judgement'\n",
    "    ],\n",
    "    task_manager = task_manager,\n",
    "    log_samples = True, # set to False\n",
    "    batch_size = 20,\n",
    "    #limit = 15, # using the full dataset\n",
    "    random_seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24e61eb4-8f6b-4bb5-a1b3-8d3a614efc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del results_bbh['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09a5f66f-4735-442b-9aff-178345e7cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_bbh_causal_json', 'w') as f:\n",
    "    json.dump(results_bbh, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3be72-6571-45c4-b5ff-6ff45b15de68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 3. multifin (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f78af02e-806e-44c8-bd35-9d7351dbc271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.run_multifin_benchmark(few_shot_instruction, few_shot_response, bench_questions, bench_labels)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_multifin_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0216da30-b411-4b5a-bed6-45311522ab27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534 questions to answer\n",
      "Number completed: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 100 150 200 250 300 350 400 450 500 "
     ]
    }
   ],
   "source": [
    "results_multifin = run_multifin_benchmark(few_shot_instruction, few_shot_response, \n",
    "                                          bench_questions, bench_labels, multifin_prompt_instruction\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fea43c8-d376-4f67-a95d-6a271206bede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samples', 'benchmark_accuracy'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_multifin.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e029129-82e4-42b9-b450-4b671f7a91d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(pre_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "466ee541-96ff-4b44-bb7a-bff5178a0f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('results_multifin', 'w') as f:\n",
    "    json.dump(results_multifin, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11888b44-fc03-4bd4-8c8e-9e7b1ac95c0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4. Run on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4f2137e-75cf-4a6b-bc40-71eeb8060d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shot_fin_instruction = list(few_shot_fin['Instruction'])\n",
    "few_shot_fin_response = list(few_shot_fin['Response'])\n",
    "bench_questions_fin = list(test['question']) # for running benchmark\n",
    "bench_labels_fin = list(test['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f7728d-4c2a-44c2-a175-7215240fb2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970 questions to answer\n",
      "Number completed: \n",
      "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 "
     ]
    }
   ],
   "source": [
    "# results after one epoch and learning_rate = 0.001\n",
    "results_finPhrase_testset_pre = run_multifin_benchmark(few_shot_fin_instruction, few_shot_fin_response, \n",
    "                                                        bench_questions_fin, bench_labels_fin) # run with default prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94ebad35-76ee-476b-beba-f86b77e99ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26597938144329897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4657216494845361"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results_finPhrase_testset_pre['benchmark_accuracy_exact'])\n",
    "results_finPhrase_testset_pre['benchmark_accuracy_proximate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a00dd04b-b38e-4658-b1c2-f5213ea9869d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(pre_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5e01a09-350d-4c50-84b3-91f185ff5552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('results_finphraseTest', 'w') as f:\n",
    "    json.dump(results_finPhrase_testset_pre, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82306b54-33ce-4ddc-9e14-d8fb9828ea17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Look at results and examples for each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba515d-c2dc-428a-851a-80d2aa932af8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 1. arc_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5195b3e7-432a-4c44-84f1-1e66079426e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Results:\n",
      " {'arc_easy': {'alias': 'arc_easy', 'acc,none': 0.4503367003367003, 'acc_stderr,none': 0.010209047724374148, 'acc_norm,none': 0.4065656565656566, 'acc_norm_stderr,none': 0.010079056419223537}} \n",
      "\n",
      "--------------------\n",
      "Sample 1:\n",
      "\n",
      "Answer Key:  A\n",
      "\n",
      "Argument:  [('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' Sunlight is the source of energy for nearly all ecosystems.'), ('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' Most ecosystems are found on land instead of in water.'), ('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' Carbon dioxide is more available than other gases.'), ('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' The producers in all ecosystems are plants.')]\n",
      "\n",
      "Model Response:  [[(-26.11143684387207, False)], [(-37.07452392578125, False)], [(-31.142784118652344, False)], [(-34.876827239990234, False)]]\n",
      "\n",
      "Model accuracy for this question:  1.0\n",
      "\n",
      "--------------------\n",
      "Sample 2:\n",
      "\n",
      "Answer Key:  B\n",
      "\n",
      "Argument:  [('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' safety goggles'), ('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' breathing mask'), ('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' rubber gloves'), ('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' lead apron')]\n",
      "\n",
      "Model Response:  [[(-17.515085220336914, False)], [(-16.706100463867188, False)], [(-11.285815238952637, False)], [(-28.14014434814453, False)]]\n",
      "\n",
      "Model accuracy for this question:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------\\nResults:\\n\", results_arc['results'], \"\\n\")\n",
    "print(\"--------------------\\nSample 1:\\n\")\n",
    "print(\"Answer Key: \", results_arc['samples']['arc_easy'][0]['doc']['answerKey'])\n",
    "print(\"\\nArgument: \", results_arc['samples']['arc_easy'][0]['arguments'])\n",
    "print(\"\\nModel Response: \", results_arc['samples']['arc_easy'][0]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_arc['samples']['arc_easy'][0]['acc'])\n",
    "\n",
    "print(\"\\n--------------------\\nSample 2:\\n\")\n",
    "print(\"Answer Key: \", results_arc['samples']['arc_easy'][1]['doc']['answerKey'])\n",
    "print(\"\\nArgument: \", results_arc['samples']['arc_easy'][1]['arguments'])\n",
    "print(\"\\nModel Response: \", results_arc['samples']['arc_easy'][1]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_arc['samples']['arc_easy'][1]['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63552a2-229f-4b40-8f7e-81b67ef7896b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 2. bbh_zeroshot_causal_judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4eea9913-79b8-4427-8059-89c3c710b3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Results:\n",
      " {'bbh_zeroshot_causal_judgement': {'alias': 'bbh_zeroshot_causal_judgement', 'exact_match,strict-match': np.float64(0.0), 'exact_match_stderr,strict-match': 0.0, 'exact_match,flexible-extract': np.float64(0.31016042780748665), 'exact_match_stderr,flexible-extract': 0.033916480026151344}} \n",
      "\n",
      "--------------------\n",
      "Sample 1:\n",
      "\n",
      "Answer Key:  No\n",
      "\n",
      "Argument:  [('Answer questions about causal attribution.\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\\nOptions:\\n- Yes\\n- No\\nA:', {'until': ['</s>', 'Q:', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0})]\n",
      "\n",
      "Model Response:  [[' The black wire is supposed to touch the battery, but it is not supposed to touch the battery.\\nB: The red wire is supposed to remain in some other part of the machine.\\nC: The black wire is supposed to remain in some other part of the machine.\\nD: The black wire is supposed to remain in some other part of the machine.\\n\\nOption:\\n- Yes\\n- No\\nA: The black wire is supposed to touch the battery, but it is not supposed to touch the battery.\\nB: The red wire is supposed to remain in some other part of the machine.\\nC: The black wire is supposed to remain in some other part of the machine.\\nD: The black wire is supposed to remain in some other part of the machine.\\n\\nOption:\\n- Yes\\n- No\\nA: The black wire is supposed to touch the battery, but it is not supposed to touch the battery.\\nB: The red wire is supposed to remain in some other part of the machine.\\nC: The black wire is supposed to remain in some other part of the machine.\\nD: The black wire is supposed to remain in some other part of the machine.\\n\\nOption:\\n- Yes\\n']]\n",
      "\n",
      "Model accuracy for this question:  0.0\n",
      "--------------------\n",
      "Sample 2:\n",
      "\n",
      "Answer Key:  No\n",
      "\n",
      "Argument:  [(\"Answer questions about causal attribution.\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nLong ago, when John was only 17 years old, he got a job working for a large manufacturing company. He started out working on an assembly line for minimum wage, but after a few years at the company, he was given a choice between two line manager positions. He could stay in the woodwork division, which is where he was currently working. Or he could move to the plastics division. John was unsure what to do because he liked working in the woodwork division, but he also thought it might be worth trying something different. He finally decided to switch to the plastics division and try something new. For the last 30 years, John has worked as a production line supervisor in the plastics division. After the first year there, the plastics division was moved to a different building with more space. Unfortunately, through the many years he worked there, John was exposed to asbestos, a highly carcinogenic substance. Most of the plastics division was quite safe, but the small part in which John worked was exposed to asbestos fibers. And now, although John has never smoked a cigarette in his life and otherwise lives a healthy lifestyle, he has a highly progressed and incurable case of lung cancer at the age of 50. John had seen three cancer specialists, all of whom confirmed the worst: that, except for pain, John's cancer was untreatable and he was absolutely certain to die from it very soon (the doctors estimated no more than 2 months). Yesterday, while John was in the hospital for a routine medical appointment, a new nurse accidentally administered the wrong medication to him. John was allergic to the drug and he immediately went into shock and experienced cardiac arrest (a heart attack). Doctors attempted to resuscitate him but he died minutes after the medication was administered. Did John's job cause his premature death?\\nOptions:\\n- Yes\\n- No\\nA:\", {'until': ['</s>', 'Q:', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0})]\n",
      "\n",
      "Model Response:  [[' Yes\\nB: No\\nC: No\\nD: No\\n\\n']]\n",
      "\n",
      "Model accuracy for this question:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------\\nResults:\\n\", results_bbh['results'], \"\\n\")\n",
    "print(\"--------------------\\nSample 1:\\n\")\n",
    "print(\"Answer Key: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['doc']['target'])\n",
    "print(\"\\nArgument: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['arguments'])\n",
    "print(\"\\nModel Response: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['exact_match'])\n",
    "\n",
    "print(\"--------------------\\nSample 2:\\n\")\n",
    "print(\"Answer Key: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['doc']['target'])\n",
    "print(\"\\nArgument: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['arguments'])\n",
    "print(\"\\nModel Response: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['exact_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc29f32-8bcc-4fd5-9fa4-bfa94786c4bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 3. multifin - custom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90cf7822-c578-450d-8dcd-cb61e323e502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09363295880149813"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_multifin['benchmark_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30a48e72-57d3-407b-8955-92258a449e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_output': [{'generated_text': \"In this task, you're working with English headlines. \\n                        This dataset is made up of real-world article headlines from a large accounting firm's websites. \\n                        Your objective is to categorize each headline as one of the predefined categories.\\n                        Categories: ['Finance', 'Technology', 'Tax & Accounting', 'Business & Management', \\n                        'Government & Controls', 'Industry']. Reply with one word only. \\n                        Following are a few examples of similar text and the correct category.\\n                        Provide the answer for the last body of text.\\nText: {PwC participated in the opening of the market for trading at the London Stock Exchange}\\nAnswer: {Finance}\\nText: {Removal of the annual limit of the pension and disability contributions â€“ employment costs are to increase}\\nAnswer: {Finance}\\nText: {Technology has reduced the tax compliance burden on business, but where will the demands for increased data lead?}\\nAnswer: {Technology}\\nText: {Information Security Strategy}\\nAnswer: {Technology}\\nText: {Tax reporting & strategy}\\nAnswer: {Tax & Accounting}\\nText: {Taxes in Iceland}\\nAnswer: {Tax & Accounting}\\nText: {The evolving nature of the green agenda}\\nAnswer: {Business & Management}\\nText: {Generational transfer}\\nAnswer: {Business & Management}\\nText: {SOX and internal control}\\nAnswer: {Government & Controls}\\nText: {Government and Public Sector}\\nAnswer: {Government & Controls}\\nText: {PwC announces the short list of nominees for the Healthy Sense Book Award}\\nAnswer: {Industry}\\nText: {Direct to Consumer}\\nAnswer: {Industry}\\nText: {Deal summary Helen Oy / Infratek Finland Oy}\\nAnswer: {Pw\"}],\n",
       " 'formatted_response': {'full_text': ['Text: {Deal summary Helen Oy / Infratek Finland Oy}',\n",
       "   'Answer: {Pw'],\n",
       "  'model_answer': 'Pw'},\n",
       " 'model_answer': 'Pw',\n",
       " 'correct_answer': 'Finance',\n",
       " 'accuracy_question': 0.0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_multifin['samples'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11c4a9d1-a41a-4d5d-aeda-5907bb9bc040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_output': [{'generated_text': \"In this task, you're working with English headlines. \\n                        This dataset is made up of real-world article headlines from a large accounting firm's websites. \\n                        Your objective is to categorize each headline as one of the predefined categories.\\n                        Categories: ['Finance', 'Technology', 'Tax & Accounting', 'Business & Management', \\n                        'Government & Controls', 'Industry']. Reply with one word only. \\n                        Following are a few examples of similar text and the correct category.\\n                        Provide the answer for the last body of text.\\nText: {PwC participated in the opening of the market for trading at the London Stock Exchange}\\nAnswer: {Finance}\\nText: {Removal of the annual limit of the pension and disability contributions â€“ employment costs are to increase}\\nAnswer: {Finance}\\nText: {Technology has reduced the tax compliance burden on business, but where will the demands for increased data lead?}\\nAnswer: {Technology}\\nText: {Information Security Strategy}\\nAnswer: {Technology}\\nText: {Tax reporting & strategy}\\nAnswer: {Tax & Accounting}\\nText: {Taxes in Iceland}\\nAnswer: {Tax & Accounting}\\nText: {The evolving nature of the green agenda}\\nAnswer: {Business & Management}\\nText: {Generational transfer}\\nAnswer: {Business & Management}\\nText: {SOX and internal control}\\nAnswer: {Government & Controls}\\nText: {Government and Public Sector}\\nAnswer: {Government & Controls}\\nText: {PwC announces the short list of nominees for the Healthy Sense Book Award}\\nAnswer: {Industry}\\nText: {Direct to Consumer}\\nAnswer: {Industry}\\nText: {Creating value beyond the deal: Private Equity}\\nAnswer: {Finance\"}],\n",
       " 'formatted_response': {'full_text': ['Text: {Creating value beyond the deal: Private Equity}',\n",
       "   'Answer: {Finance'],\n",
       "  'model_answer': 'Finance'},\n",
       " 'model_answer': 'Finance',\n",
       " 'correct_answer': 'Finance',\n",
       " 'accuracy_question': 1.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_multifin['samples'][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947fcbc-4b06-4f9e-9602-9a2ad9c9d1de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4. Test set of financial phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7907668b-7ea1-43aa-954f-8ce4d482813b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26597938144329897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4657216494845361"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results_finPhrase_testset_pre['benchmark_accuracy_exact'])\n",
    "results_finPhrase_testset_pre['benchmark_accuracy_proximate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3c76ad2-0586-4d89-966c-8c700111a896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_output': [{'generated_text': \"You are a financial expert that analyzes financial news reports. Each report contains numbers and \\n                                    qualitative descriptions of a subject's financial performance. \\n                                    Your job is to classify the sentiment of each report as one of the predefined categories.\\n                                    Sentiment categories: ['positive', 'neutral', 'negative']. Reply with one word only.\\n                                    Following are a few examples of similar text and the correct category. \\n                                    Provide the sentiment category for the last body of text.\\nText: {Publishing Sweden 's operating loss was EUR 1.1 mn in Q1 of 2009 , compared to a profit of EUR 0.6 mn a year ago .}\\nAnswer: {negative}\\nText: {The share capital of Alma Media Corporation (business ID 1944757-4)is EUR 45,031,513.80 and it is divided into 75,052,523 shares .}\\nAnswer: {neutral}\\nText: {The company plans to expand into the international market through its subsidiaries and distributors from 2011 onwards .}\\nAnswer: {positive}\\nText: {Nokia s U.S. shares were 3.3 percent lower at $ 12.73 by 1750 GMT .}\\nAnswer: {negative}\\nText: {The parties have agreed not to disclose the transaction value .}\\nAnswer: {neutral}\\nText: {Rapala said it estimates it will make savings of 1-2 mln eur a year by centralising its French operations at one site .}\\nAnswer: {positive}\\nText: {At a press conference , Lattelecom board chairman Nils Melngailis explained that Blackstone had been chosen for its experience in the IT sector as well as its financial strength .}\\nAnswer: {positive}\\nText: {The dividend will come on top of the 0.45 eur on A shares and 0.43 on K shares it has already paid on last year 's accounts .}\\nAnswer: {neutral}\\nText: {Korhonen was dismissed from her post of editor in chief of the group 's newspaper Lapin Kansa in December 2008 .}\\nAnswer: {negative}\\nText: {Growth is expected to continue in 2008 . Choose one of the following words: positive, neutral, negative}\\nAnswer: {negative}\"}],\n",
       " 'formatted_response': {'full_text': ['Text: {Growth is expected to continue in 2008 . Choose one of the following words: positive, neutral, negative}',\n",
       "   'Answer: {negative}'],\n",
       "  'model_answer': 'negative'},\n",
       " 'model_answer': 'negative',\n",
       " 'correct_answer': 'positive',\n",
       " 'accuracy_question': 0.0,\n",
       " 'accuracy_proximity_question': 0.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_finPhrase_testset_pre['samples'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8850ae41-6def-4832-8286-7b54103212da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_output': [{'generated_text': \"You are a financial expert that analyzes financial news reports. Each report contains numbers and \\n                                    qualitative descriptions of a subject's financial performance. \\n                                    Your job is to classify the sentiment of each report as one of the predefined categories.\\n                                    Sentiment categories: ['positive', 'neutral', 'negative']. Reply with one word only.\\n                                    Following are a few examples of similar text and the correct category. \\n                                    Provide the sentiment category for the last body of text.\\nText: {Publishing Sweden 's operating loss was EUR 1.1 mn in Q1 of 2009 , compared to a profit of EUR 0.6 mn a year ago .}\\nAnswer: {negative}\\nText: {The share capital of Alma Media Corporation (business ID 1944757-4)is EUR 45,031,513.80 and it is divided into 75,052,523 shares .}\\nAnswer: {neutral}\\nText: {The company plans to expand into the international market through its subsidiaries and distributors from 2011 onwards .}\\nAnswer: {positive}\\nText: {Nokia s U.S. shares were 3.3 percent lower at $ 12.73 by 1750 GMT .}\\nAnswer: {negative}\\nText: {The parties have agreed not to disclose the transaction value .}\\nAnswer: {neutral}\\nText: {Rapala said it estimates it will make savings of 1-2 mln eur a year by centralising its French operations at one site .}\\nAnswer: {positive}\\nText: {At a press conference , Lattelecom board chairman Nils Melngailis explained that Blackstone had been chosen for its experience in the IT sector as well as its financial strength .}\\nAnswer: {positive}\\nText: {The dividend will come on top of the 0.45 eur on A shares and 0.43 on K shares it has already paid on last year 's accounts .}\\nAnswer: {neutral}\\nText: {Korhonen was dismissed from her post of editor in chief of the group 's newspaper Lapin Kansa in December 2008 .}\\nAnswer: {negative}\\nText: {The market share of Volkswagen passenger cars in Finland was 10.1 percent , Audi had a market share of 3.1 percent and Seat 's share was 0.9 percent . Choose one of the following words: positive, neutral, negative}\\nAnswer: {Negative\"}],\n",
       " 'formatted_response': {'full_text': [\"Text: {The market share of Volkswagen passenger cars in Finland was 10.1 percent , Audi had a market share of 3.1 percent and Seat 's share was 0.9 percent . Choose one of the following words: positive, neutral, negative}\",\n",
       "   'Answer: {Negative'],\n",
       "  'model_answer': 'Negative'},\n",
       " 'model_answer': 'Negative',\n",
       " 'correct_answer': 'neutral',\n",
       " 'accuracy_question': 0.0,\n",
       " 'accuracy_proximity_question': 0.5}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_finPhrase_testset_pre['samples'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a17ed-0d9e-4bc9-977f-a99eed6c2eba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b241985-5563-425a-b44b-78228667851e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare for prompt tuning - from HW 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f7c81-bcb4-41c8-8825-f5e6ee65055e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### instruct column already added to the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad94584-13d9-4236-b20f-ed7cc10993ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Prepare model and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2786ed2a-8161-4358-910f-09884469c774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3867, 3)\n",
      "(970, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac4e9787-a6ad-478d-a9cc-d8819d2e9086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3867/3867 [00:00<00:00, 9361.26 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 970/970 [00:00<00:00, 7611.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train = Dataset.from_pandas(train)\n",
    "val = Dataset.from_pandas(test)\n",
    "train = train.map(lambda samples: tokenizer(samples['instruct']), batched = True)\n",
    "val = val.map(lambda samples: tokenizer(samples['instruct']), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bc87883-b4fd-4ad5-a9e7-acaca45a11b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(formatted_prompt, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         output_attentions,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:441\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    439\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 441\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    443\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:368\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 368\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    370\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/transformers/pytorch_utils.py:124\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    123\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 124\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# very poorly formatted response from the model\n",
    "formatted_prompt = f\"Q: {train[0]['question']}\\n\\nA: \"\n",
    "inputs = tokenizer.encode(formatted_prompt, return_tensors = \"pt\").to(model.device)\n",
    "output = model.generate(inputs, max_new_tokens = 150, pad_token_id = tokenizer.pad_token_id, do_sample = False)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84a8f4-a693-4e01-a704-2c4c62abbe90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### PromptTuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dff05d45-b5e1-4c44-ad14-f36072ae0821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,680 || all params: 124,449,792 || trainable%: 0.0062\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "generation_config = PromptTuningConfig(\n",
    "    task_type = TaskType.CAUSAL_LM, # task type\n",
    "    prompt_tuning_init = PromptTuningInit.RANDOM, # init virtual tokens\n",
    "    num_virtual_tokens = 10,\n",
    "    tokenizer_name_or_path = \"openai-community/gpt2-xl\")\n",
    "model = get_peft_model(model, generation_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c253b-1cfe-442a-9706-265891cdb321",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ATTEMPT 1: Train - 1 epoch, learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a1ab2-c9f7-42ed-b02a-7495cf37dd54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d142457e-d362-4370-bdd8-d025442289eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### start with 1 epoch\n",
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate = 0.001, epochs = 1, eval_steps = 150):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, \n",
    "        auto_find_batch_size = True,\n",
    "        learning_rate = learning_rate, \n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps, \n",
    "        save_steps = eval_steps, \n",
    "        load_best_model_at_end = True)\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a1b471a-65f1-4b2a-9d0a-566396e0c26a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dcc7qe/LLM_project/gpt2_instruct/trained_models'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f87d307e-05f6-4fe0-8c03-4b404ff74293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = create_training_arguments(trained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3649139e-ec3d-4a74-9e08-1dae40301b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# creates a trainer object for training\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, \n",
    "            mlm = False))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cd5a17c-8e90-46ac-aafe-ab77a8018ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa992de-026e-4583-a380-6f457ca4a436",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8bfdadb6-ff9e-4a9a-b5e3-e31dd63d9ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='484' max='484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [484/484 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.821000</td>\n",
       "      <td>2.547609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.470700</td>\n",
       "      <td>2.350604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.312200</td>\n",
       "      <td>2.208324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=484, training_loss=2.5189645782975125, metrics={'train_runtime': 33.6317, 'train_samples_per_second': 114.981, 'train_steps_per_second': 14.391, 'total_flos': 136190306304000.0, 'train_loss': 2.5189645782975125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fd23c-e1b4-49ae-875c-d99a41bfa871",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ATTEMPT 2: Train - 2 epochs, learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa552e-cdfa-442d-9d58-e2050678923e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "042cd264-9713-4b04-a6b9-ce046a0f81b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e9dad89-fe14-46af-b30d-9ea14b507396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50260, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_choice = \"vicgalle/gpt2-open-instruct-v1\" # used throughout the notebook\n",
    "#model_choice = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = import_model_token(model_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebc9ea-c3a6-4e4d-9d1c-9ea2d9a69605",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a539919d-48c7-44fe-ab53-7de6a72bef84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### start with 1 epoch\n",
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate = 0.001, epochs = 2, eval_steps = 150):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, \n",
    "        auto_find_batch_size = True,\n",
    "        learning_rate = learning_rate, \n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps, \n",
    "        save_steps = eval_steps, \n",
    "        load_best_model_at_end = True)\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7e0845ce-b295-478c-a775-32f48ab74fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dcc7qe/LLM_project/gpt2_instruct/trained_models'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "228e55ae-6fa9-4ae8-8c39-eadf80468f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = create_training_arguments(trained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2a4bc07-eabf-4a54-a929-303168fecc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# creates a trainer object for training\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, \n",
    "            mlm = False))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fa55ebbe-4d75-4523-8f25-ba9207d62f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddc0ca-fbc5-407a-8338-4cd11e28579d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b346be3f-3e68-42f0-a7b5-579dd8f87792",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 01:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.849400</td>\n",
       "      <td>2.603967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.547000</td>\n",
       "      <td>2.460616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.442300</td>\n",
       "      <td>2.348077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.811700</td>\n",
       "      <td>2.388314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.573300</td>\n",
       "      <td>2.321527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.515200</td>\n",
       "      <td>2.279105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=968, training_loss=2.078559213433384, metrics={'train_runtime': 65.252, 'train_samples_per_second': 118.525, 'train_steps_per_second': 14.835, 'total_flos': 272298958848000.0, 'train_loss': 2.078559213433384, 'epoch': 2.0})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db4eee-115f-4ec3-bbe8-bcd3d07c190e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ATTEMPT 3: Train - 1 epochs, learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e4361-0488-468d-9774-3cc2b8e09469",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cb8de60-494a-4620-9c26-2d80fd15e1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9cf6068-0682-42fa-a405-3099fa2523ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61333098-2231-423b-bbee-7bc74c015a91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50260, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_choice = \"vicgalle/gpt2-open-instruct-v1\" # used throughout the notebook\n",
    "#model_choice = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = import_model_token(model_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2673b39e-b343-4d5d-9fca-c9b2dd5e65fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d91ae1e6-88d4-49f1-b3ec-aa33ae3fd66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### start with 1 epoch\n",
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate = 0.0001, epochs = 1, eval_steps = 150):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, \n",
    "        auto_find_batch_size = True,\n",
    "        learning_rate = learning_rate, \n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps, \n",
    "        save_steps = eval_steps, \n",
    "        load_best_model_at_end = True)\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c791d504-0134-46df-8d0f-ec0880ddc226",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dcc7qe/LLM_project/gpt2_instruct/trained_models'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea8569e9-dd69-4ad1-b79c-c4b280e7a918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = create_training_arguments(trained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d3cc5b7-6a57-4677-92e4-35e79322f024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# creates a trainer object for training\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, \n",
    "            mlm = False))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c324ff76-eb51-445d-bb21-28d82ea1c2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5121d-f2e9-4fa9-8b09-7640a7dfcccd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a833a1bf-9d0c-4b76-96aa-65be680e7b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='484' max='484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [484/484 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.276600</td>\n",
       "      <td>3.516257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.981600</td>\n",
       "      <td>3.401589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.894900</td>\n",
       "      <td>3.421430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=484, training_loss=3.037598759674829, metrics={'train_runtime': 29.3164, 'train_samples_per_second': 131.906, 'train_steps_per_second': 16.51, 'total_flos': 113528836224000.0, 'train_loss': 3.037598759674829, 'epoch': 1.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0cba1616-ec63-4102-acb7-1b99e23d0e76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='484' max='484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [484/484 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.463700</td>\n",
       "      <td>2.266076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.218400</td>\n",
       "      <td>2.163368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.159300</td>\n",
       "      <td>2.123967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=484, training_loss=2.273552082786875, metrics={'train_runtime': 33.5104, 'train_samples_per_second': 115.397, 'train_steps_per_second': 14.443, 'total_flos': 135821333376000.0, 'train_loss': 2.273552082786875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # yesterday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cde30-48d2-44f5-be53-494b7601be60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### SAVE MODEL - good one but not the best anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "078b4829-7a89-4e2a-bdb2-b603775410d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: According to CEO Matti Perkonoja of the parent company HKScan , the company 's performance in the first quarter of 2010 has remained clearly below the level of the corresponding period in 2009 . Choose one of the following words: positive, neutral, negative\n",
      "\n",
      "A:  positive, neutral, negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n",
      "Response: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = f\"Q: {train[0]['question']}\\n\\nA: \"\n",
    "inputs = tokenizer.encode(formatted_prompt, return_tensors = \"pt\").to(model.device)\n",
    "output = model.generate(inputs, max_new_tokens = 150, pad_token_id = tokenizer.pad_token_id, do_sample = False)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6542e85-ac71-4f87-a35e-9ffd5f3b8590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dcc7qe/LLM_project/gpt2_instruct/trained_models/best_model'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bcb830a-9f85-4499-9334-48021eb362ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(best_model_results_dir) # saved at one point as the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f74de-733b-4170-92cc-4e2973879f99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ATTEMPT 4: Try LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813f634-cc43-41ba-a396-a9d7fbf36642",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d21f2501-c2d6-4d9b-8486-17429f36c1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e145944d-a7a8-4cc0-945c-b58ae2c12e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50260, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_choice = \"vicgalle/gpt2-open-instruct-v1\" # used throughout the notebook\n",
    "#model_choice = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = import_model_token(model_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e7cd1-6b11-48ff-a11c-5591754a5ad7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### prep lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5eb1125c-a27f-4a17-bbff-94112144b5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,077,888 || all params: 131,520,000 || trainable%: 5.3816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dcc7qe/llm_course/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up LoraConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "lora_config = LoraConfig(\n",
    "    r = LORA_R,\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = LORA_DROPOUT,\n",
    "    bias = \"none\", \n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = ['c_fc', \n",
    "                      'c_proj'], # just use a couple of the mlp layers\n",
    ")\n",
    "model = get_peft_model(model, lora_config) \n",
    "model.print_trainable_parameters() ### this seems large - surprised that it is larger than llama used in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3495edf-3589-43f3-873e-85a6c59a4a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "##### reference: notebook from lecture\n",
    "\n",
    "# creates trainer object for training \n",
    "def create_training_arguments_lora(path, learning_rate = 0.0001, epochs = 2, eval_steps = 150):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, \n",
    "        auto_find_batch_size = True,\n",
    "        learning_rate = learning_rate,\n",
    "        num_train_epochs = epochs,\n",
    "        logging_steps = eval_steps,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "        load_best_model_at_end = True)\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1158d756-2551-411e-bece-deebca3a6763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = create_training_arguments_lora(trained_model_dir + '/LoRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "57b161c7-1ff7-4393-92da-e92505a7e4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create trainer (same as what we did in HW6)\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# creates a trainer object for training\n",
    "def create_trainer_lora(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, \n",
    "            mlm = False))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f90f653e-5f63-46fb-be13-4c05b6c58799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer_lora(model, training_args, train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fd235-bad8-4e74-9261-f54d512e14f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Run LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a7292ee0-93da-434e-8eea-f80b805584e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 00:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.745500</td>\n",
       "      <td>3.722314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.339100</td>\n",
       "      <td>3.654149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.263000</td>\n",
       "      <td>3.634432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.204600</td>\n",
       "      <td>3.639663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.164900</td>\n",
       "      <td>3.625385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.645490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=968, training_loss=3.287555726106502, metrics={'train_runtime': 53.5761, 'train_samples_per_second': 144.355, 'train_steps_per_second': 18.068, 'total_flos': 246094221542400.0, 'train_loss': 3.287555726106502, 'epoch': 2.0})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963664d-e263-4dfd-90d3-050252a5040c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ATTEMPT 5: epochs = 4, learning rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0392b5-1600-4972-90d2-1ecd7eabbd13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57c447e6-785b-4bbd-8cf8-b33f5a92b822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50260, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "model_choice = \"vicgalle/gpt2-open-instruct-v1\" # used throughout the notebook\n",
    "model, tokenizer = import_model_token(model_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ae53d-2936-42fb-8145-dee62846e6f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### prep trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cae50847-879a-4abd-b9c8-5dcfde23c5de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### start with 1 epoch\n",
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate = 0.0001, epochs = 4, eval_steps = 150):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, \n",
    "        auto_find_batch_size = True,\n",
    "        learning_rate = learning_rate, \n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps, \n",
    "        save_steps = eval_steps, \n",
    "        load_best_model_at_end = True)\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a01fadff-e1b1-4e76-9ccf-5a091e82362d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = create_training_arguments(trained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6afc8e81-de89-4b14-9191-dc647d56a465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# creates a trainer object for training\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, \n",
    "            mlm = False))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8067710e-d7c8-498c-b2b7-0f47ad5fa34a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48968848-9d81-4c7c-b08a-d52366578332",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run - rate = 0.0001, 4 epochs -> 2nd best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a630756b-249c-48ff-b301-e6f9d11626e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1936' max='1936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1936/1936 02:31, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.241900</td>\n",
       "      <td>3.523757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.003300</td>\n",
       "      <td>3.476349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.879400</td>\n",
       "      <td>3.475524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.485100</td>\n",
       "      <td>3.557681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.356100</td>\n",
       "      <td>3.515241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.366600</td>\n",
       "      <td>3.577992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.135500</td>\n",
       "      <td>3.677817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.036900</td>\n",
       "      <td>3.697888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.024900</td>\n",
       "      <td>3.747730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.948700</td>\n",
       "      <td>3.863908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.828000</td>\n",
       "      <td>3.911299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.832600</td>\n",
       "      <td>3.902198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1936, training_loss=2.3080819342747207, metrics={'train_runtime': 151.3389, 'train_samples_per_second': 102.208, 'train_steps_per_second': 12.792, 'total_flos': 449023212288000.0, 'train_loss': 2.3080819342747207, 'epoch': 4.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf456522-2625-4b20-8a3b-aa5eb2aae8e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970 questions to answer\n",
      "Number completed: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 accuracy-exact: 0.48556701030927835 \n",
      "\n",
      "accuracy-proximate: 0.6371134020618556 \n",
      "\n",
      "Model:  {'full_text': ['Text: {Residents access to the block is planned to be from Aleksandri Street . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': ['Answer: neutral', 'Text'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': [\"Text: {The terms of the aforementioned funding are considerably below the Bank 's current CDS levels in the market and have a maturity ranging from 1 to 7.5 years . Choose one of the following words: positive, neutral, negative}\", 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  negative \n",
      "\n",
      "Model:  {'full_text': ['Text: {Fortum had previously bought the state-held stake in TGK-10 from RAO UES during its reform . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': ['Answer: negative', 'Text'], 'model_answer': 'negative'} \n",
      " Correct:  neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_fin = few_shot_fin.sample(frac = 1)\n",
    "few_shot_fin_instruction = list(few_shot_fin['Instruction'])\n",
    "few_shot_fin_response = list(few_shot_fin['Response'])\n",
    "bench_questions_fin = list(test['question']) # for running benchmark\n",
    "bench_labels_fin = list(test['answer'])\n",
    "results_ = run_multifin_benchmark(few_shot_fin_instruction, few_shot_fin_response, \n",
    "                                    bench_questions_fin, bench_labels_fin)\n",
    "print(\"accuracy-exact:\", results_['benchmark_accuracy_exact'], \"\\n\")\n",
    "print(\"accuracy-proximate:\", results_['benchmark_accuracy_proximate'], \"\\n\")\n",
    "for i in range(5): print(\"Model: \", results_['samples'][i]['formatted_response'], \"\\n\", \"Correct: \", results_['samples'][i]['correct_answer'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2491ff-2bd4-43ce-9ff7-3abc163fff56",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 2nd BEST - saved at one point, then overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc41416f-434a-4a56-9129-a8e77c2bab24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dcc7qe/LLM_project/gpt2_instruct/trained_models/best_model'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb29e330-ce43-40b2-b2ae-ece5a51dd750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(best_model_results_dir) # save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc710679-ad30-4632-bba4-217c552a464f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **BEST MODEL - ATTEMPT 6**: rate = 0.0001, epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb4e8de1-b7f3-4f55-a6c9-c14614aead3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### start with 1 epoch\n",
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate = 0.0001, epochs = 5, eval_steps = 150):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = path, \n",
    "        auto_find_batch_size = True,\n",
    "        learning_rate = learning_rate, \n",
    "        num_train_epochs = epochs, \n",
    "        logging_steps = eval_steps,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps, \n",
    "        save_steps = eval_steps, \n",
    "        load_best_model_at_end = True)\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b462f46a-1074-4c4e-9d6b-4653085f92fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = create_training_arguments(trained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd3cf3b7-29b7-4c23-bec8-6d47efc262c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# creates a trainer object for training\n",
    "def create_trainer(model, training_args, train_dataset, eval_dataset):\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, \n",
    "            mlm = False))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f050912c-33a9-4075-b3e8-4655a72f696e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(model, training_args, train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f88f42-c845-4613-954c-37f8c6e881cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **BEST** Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f74f654-cd15-4e89-ac31-694a036a5b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2420' max='2420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2420/2420 02:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.258000</td>\n",
       "      <td>3.539656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.933200</td>\n",
       "      <td>3.429196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.895200</td>\n",
       "      <td>3.363705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.544000</td>\n",
       "      <td>3.540796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.344600</td>\n",
       "      <td>3.573031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.332600</td>\n",
       "      <td>3.582210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.136300</td>\n",
       "      <td>3.694432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.024400</td>\n",
       "      <td>3.737149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.007000</td>\n",
       "      <td>3.735977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.923500</td>\n",
       "      <td>3.904024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.764600</td>\n",
       "      <td>3.955567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.788300</td>\n",
       "      <td>3.980736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.757800</td>\n",
       "      <td>4.038834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.594700</td>\n",
       "      <td>4.158232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.603600</td>\n",
       "      <td>4.163863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.639600</td>\n",
       "      <td>4.161632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2420, training_loss=2.155215175092713, metrics={'train_runtime': 152.6623, 'train_samples_per_second': 126.652, 'train_steps_per_second': 15.852, 'total_flos': 562026912768000.0, 'train_loss': 2.155215175092713, 'epoch': 5.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7a854b7-3ee0-4347-b912-0fa55dfc6b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970 questions to answer\n",
      "Number completed: \n",
      "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 \n",
      "\n",
      "accuracy-exact: 0.5288659793814433 \n",
      "\n",
      "accuracy-proximate: 0.663659793814433 \n",
      "\n",
      "Model:  {'full_text': ['Text: {`` BG Crane has been a strong partner for Hiab in Australia for many years . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': ['Text: {The aim is to increase sales by at least one fifth in 2006 . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n",
      "Model:  {'full_text': ['Answer: neutral', 'Text'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': [\"Text: {It delivers a different user experience and enables us to widen the market we can address . '' Choose one of the following words: positive, neutral, negative}\", 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n",
      "Model:  {'full_text': ['Answer: neutral', 'Text'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_fin = few_shot_fin.sample(frac = 1)\n",
    "few_shot_fin_instruction = list(few_shot_fin['Instruction'])\n",
    "few_shot_fin_response = list(few_shot_fin['Response'])\n",
    "bench_questions_fin = list(test['question']) # for running benchmark\n",
    "bench_labels_fin = list(test['answer'])\n",
    "results_ = run_multifin_benchmark(few_shot_fin_instruction, few_shot_fin_response, \n",
    "                                    bench_questions_fin, bench_labels_fin)\n",
    "print(\"\\n\\naccuracy-exact:\", results_['benchmark_accuracy_exact'], \"\\n\")\n",
    "print(\"accuracy-proximate:\", results_['benchmark_accuracy_proximate'], \"\\n\")\n",
    "for i in range(5): print(\"Model: \", results_['samples'][i]['formatted_response'], \"\\n\", \"Correct: \", results_['samples'][i]['correct_answer'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f269072-e2c4-4d9a-a8e3-303e189143d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **BEST (final)** Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8adad3b-2ae3-4fbc-841b-63e55774d4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dcc7qe/LLM_project/gpt2_instruct/trained_models/best_model'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "132d81c1-cc79-4f2b-b247-3ddba7b3ab2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(best_model_results_dir) # save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d32e22f-f709-4348-8170-0c847f7b6642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(post_results_dir)\n",
    "with open('results_finphraseTest_postBEST', 'w') as f:\n",
    "    json.dump(results_, f) # this was post2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac71c0b6-7f96-4256-8701-da86e29efa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_finphraseTest_post = results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37dc5d-7d90-4cfb-a6c4-0e9c94867d78",
   "metadata": {},
   "source": [
    "### Describe results in one paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a0207-2c08-45b1-ac6d-0d485e27dce3",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "I tried the several combinations of hyperparameters including learning rate, step size, and number of epochs. The best model was attempt 6, with a learning rate of 0.0001 and 5 epochs, because the training loss continued to decrease and the validation loss did not begin to increase significantly until right before the end of the 5th epoch. I noticed early on in training that when I used a learning rate of 0.001, after just a couple epochs the validation loss increased significantly and the model output was not great. I attempted training with well over 10 epochs and have mixed results. Sometimes the training loss decreased much more slowly than with less epochs, but other times I my financial phrasebank benchmark did achieve better scores using between 10-20 epochs. The best model ended up simply using 5 epochs with a moderately low learning rate - 0.0001, and this provided improved performance on the financial phrasebank test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e41d09-cedd-4787-87df-3af3355a500d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: Assess Post-training Benchmark Performance (20 points)\n",
    "Repeat Step 2 using your trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056d30a-3a7a-4831-a213-2c0b6bbc7ae4",
   "metadata": {},
   "source": [
    "#### Run all benchmarks\n",
    "\n",
    "- arc_easy - general\n",
    "- bbh_zeroshot_causal_judgement - general\n",
    "- custom multifin - specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e803faeb-d511-495c-95fa-3ff8698e256e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up lm_eval task manager and implement\n",
    "task_manager = lm_eval.tasks.TaskManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "547d24b2-4dba-447e-83a3-3c48c457fb9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(post_results_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc377e0c-3af4-47c3-926f-c1e0205c0af7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1. arc_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30d9b6dc-d785-4152-bd5c-dd7b274a354a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:01<00:00, 1523.74it/s]\n",
      "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9501/9501 [00:07<00:00, 1268.73it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /sfs/gpfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "results_arc = lm_eval.simple_evaluate(\n",
    "    model = \"hf\", \n",
    "    model_args = {\"pretrained\": model, \n",
    "                  \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer},\n",
    "    tasks = ['arc_easy'\n",
    "    ],\n",
    "    task_manager = task_manager,\n",
    "    log_samples = True, # set to False\n",
    "    batch_size = 30,\n",
    "    #limit = 15, # using the full dataset\n",
    "    random_seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc494b40-b697-4512-8d8c-8d7e95c1da6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del results_arc['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acb08021-19cb-4c6a-930b-cf59f2e2d05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('results_arc_easy_json_post', 'w') as f:\n",
    "    json.dump(results_arc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7e793-b789-4cc1-bed1-080197fc177b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. bbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c551ed1-5903-4fb5-a673-e373a50ea941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "[Task: bbh_zeroshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "[Task: bbh_zeroshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [00:00<00:00, 1739.93it/s]\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [00:33<00:00,  5.59it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /sfs/gpfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "results_bbh = lm_eval.simple_evaluate(\n",
    "    model = \"hf\", \n",
    "    model_args = {\"pretrained\": model, \n",
    "                  \"dtype\": \"bfloat16\", \"tokenizer\": tokenizer},\n",
    "    tasks = ['bbh_zeroshot_causal_judgement'\n",
    "    ],\n",
    "    task_manager = task_manager,\n",
    "    log_samples = True, # set to False\n",
    "    batch_size = 20,\n",
    "    #limit = 15, # using the full dataset\n",
    "    random_seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c4bd61b-1c49-4556-974d-291070e7c716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del results_bbh['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "508d34a8-8d3a-4c9d-89f2-ef52b5cf0591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('results_bbh_causal_json_post', 'w') as f:\n",
    "    json.dump(results_bbh, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b347f95-b520-4010-ac62-1e28bc7501f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Multifin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb600f2f-da67-4dc6-a7d7-251ccd1c1b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534 questions to answer\n",
      "Number completed: \n",
      "50 100 150 200 250 300 350 400 450 500 "
     ]
    }
   ],
   "source": [
    "results_multifin_post = run_multifin_benchmark(few_shot_instruction, few_shot_response, \n",
    "                                          bench_questions, bench_labels, multifin_prompt_instruction\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a241ffe-f6f3-4d6a-ad0e-aa1abbdcada9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('results_multifin_post', 'w') as f:\n",
    "    json.dump(results_multifin_post, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f8460-9fd7-4044-9a21-888ba9aa8f51",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4. Fin phrase - 1 epoch, learning = 0.0001 with proper string formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e130f8-c258-44b3-bb96-2baeb8d92ef8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### variable setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e10fae-d221-4789-bd87-850aa8a0d232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "970\n",
      "970\n"
     ]
    }
   ],
   "source": [
    "few_shot_fin = few_shot_fin.sample(frac = 1)\n",
    "few_shot_fin_instruction = list(few_shot_fin['Instruction'])\n",
    "few_shot_fin_response = list(few_shot_fin['Response'])\n",
    "bench_questions_fin = list(test['question']) # for running benchmark\n",
    "bench_labels_fin = list(test['answer'])\n",
    "for l in [few_shot_fin_instruction, few_shot_fin_response, bench_questions_fin, bench_labels_fin]:\n",
    "    print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46d5cde1-806b-4fd6-9eff-c7fde94625b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'neutral',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'negative']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_fin_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec99bec-57af-47d5-a1f7-a98c22d861f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **BEST RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197ed5f-1638-4b90-a688-b45b22dda36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ran above in training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cee82-bba4-48f0-848b-0720af7454ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### after 5 epochs and rate = 0.0001\n",
    "results_finphraseTest_post = run_multifin_benchmark(few_shot_fin_instruction, few_shot_fin_response, \n",
    "                                                        bench_questions_fin, bench_labels_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32d08f1b-02b3-40c6-9553-715c76b8a420",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy-exact: 0.5288659793814433 \n",
      "\n",
      "accuracy-proximate: 0.663659793814433 \n",
      "\n",
      "Model:  {'full_text': ['Text: {`` BG Crane has been a strong partner for Hiab in Australia for many years . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': ['Text: {The aim is to increase sales by at least one fifth in 2006 . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n",
      "Model:  {'full_text': ['Answer: neutral', 'Text'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': [\"Text: {It delivers a different user experience and enables us to widen the market we can address . '' Choose one of the following words: positive, neutral, negative}\", 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n",
      "Model:  {'full_text': ['Answer: neutral', 'Text'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy-exact:\", results_['benchmark_accuracy_exact'], \"\\n\")\n",
    "print(\"accuracy-proximate:\", results_['benchmark_accuracy_proximate'], \"\\n\")\n",
    "for i in range(5): print(\"Model: \", results_['samples'][i]['formatted_response'], \"\\n\", \"Correct: \", results_['samples'][i]['correct_answer'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a69fa-8699-4b54-b20d-35297953269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### results already output in training section\n",
    "#os.chdir(post_results_dir)\n",
    "#with open('results_finphraseTest_post', 'w') as f:\n",
    "#    json.dump(results_finPhrase_testset_post, f) # this was post2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1d3c0-17cd-423a-98dd-cf835687b45e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### LoRA - didn't work for some attempts. Poor format of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93ee9d4b-c124-4552-abb8-261552b553e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970 questions to answer\n",
      "Number completed: \n",
      "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 "
     ]
    }
   ],
   "source": [
    "##### after LoRA - 2 epochs\n",
    "results_finPhrase_testset_postLoRA2 = run_multifin_benchmark(few_shot_fin_instruction, few_shot_fin_response, \n",
    "                                                        bench_questions_fin, bench_labels_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1dbce8bf-26f2-48a0-9ff4-4e4c871ae466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5154639175257731"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_finPhrase_testset_postLoRA2['benchmark_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8966485-3e6f-4242-9306-7f1eabff5586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970 questions to answer\n",
      "Number completed: \n",
      "['', '{neutral}']\n",
      "[\"'neutral'\"]\n",
      "['', 'Choose', 'one', 'of']\n",
      "['', '']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m######### results: LoRA - after 2 epoch and learning_rate = 0.0001\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results_finPhrase_testset_postLoRA \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multifin_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfew_shot_fin_instruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfew_shot_fin_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mbench_questions_fin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbench_labels_fin\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 23\u001b[0m, in \u001b[0;36mrun_multifin_benchmark\u001b[0;34m(few_shot_instruction, few_shot_response, bench_questions, bench_labels, instruction)\u001b[0m\n\u001b[1;32m     20\u001b[0m question, label \u001b[38;5;241m=\u001b[39m bench_questions[ind], bench_labels[ind]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# within the loop, query the model and receive the model answer\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m raw_output, formatted_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mfew_shot_instruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfew_shot_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mformatted_response\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#### check the answer against the label to determine accuracy\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model_answer \u001b[38;5;241m=\u001b[39m formatted_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[53], line 16\u001b[0m, in \u001b[0;36mquery_model\u001b[0;34m(model_pipe, question, instruction, few_shot_instruction, few_shot_response, formatted_response)\u001b[0m\n\u001b[1;32m     10\u001b[0m sequences \u001b[38;5;241m=\u001b[39m model_pipe(prompt,\n\u001b[1;32m     11\u001b[0m                        max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;66;03m#### **important parameter - needs to == 3 for GPT2 instruct, for a concise response\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                        do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m                        top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (formatted_response \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 16\u001b[0m     formatted_response \u001b[38;5;241m=\u001b[39m \u001b[43mformat_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: formatted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(sequences, formatted_response)\n",
      "Cell \u001b[0;32mIn[65], line 19\u001b[0m, in \u001b[0;36mformat_response\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#answer = [s for s in answer if (s != '')]\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#if (len(answer) > 2): answer = answer[:1][0]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m answer \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, answer)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "######### results: LoRA - after 2 epoch and learning_rate = 0.0001\n",
    "results_finPhrase_testset_postLoRA = run_multifin_benchmark(few_shot_fin_instruction, few_shot_fin_response, \n",
    "                                                        bench_questions_fin, bench_labels_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b5d42-acf0-422d-9ef0-a871fa74cee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2133b-7d3b-42e2-ba62-cfec140874e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1. arc_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6779d70a-98ae-40e7-aeae-fdb8a4311bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Results:\n",
      " {'arc_easy': {'alias': 'arc_easy', 'acc,none': 0.4019360269360269, 'acc_stderr,none': 0.010060521220920566, 'acc_norm,none': 0.38257575757575757, 'acc_norm_stderr,none': 0.00997283779053148}} \n",
      "\n",
      "--------------------\n",
      "Sample 1:\n",
      "\n",
      "Answer Key:  A\n",
      "\n",
      "Argument:  [('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' Sunlight is the source of energy for nearly all ecosystems.'), ('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' Most ecosystems are found on land instead of in water.'), ('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' Carbon dioxide is more available than other gases.'), ('Question: Which statement best explains why photosynthesis is the foundation of most food webs?\\nAnswer:', ' The producers in all ecosystems are plants.')]\n",
      "\n",
      "Model Response:  [[(-38.96726989746094, False)], [(-43.533409118652344, False)], [(-36.86351776123047, False)], [(-38.65947341918945, False)]]\n",
      "\n",
      "Model accuracy for this question:  0.0\n",
      "\n",
      "--------------------\n",
      "Sample 2:\n",
      "\n",
      "Answer Key:  B\n",
      "\n",
      "Argument:  [('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' safety goggles'), ('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' breathing mask'), ('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' rubber gloves'), ('Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\\nAnswer:', ' lead apron')]\n",
      "\n",
      "Model Response:  [[(-10.44250774383545, False)], [(-14.670703887939453, False)], [(-8.388999938964844, False)], [(-23.052902221679688, False)]]\n",
      "\n",
      "Model accuracy for this question:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------\\nResults:\\n\", results_arc['results'], \"\\n\")\n",
    "print(\"--------------------\\nSample 1:\\n\")\n",
    "print(\"Answer Key: \", results_arc['samples']['arc_easy'][0]['doc']['answerKey'])\n",
    "print(\"\\nArgument: \", results_arc['samples']['arc_easy'][0]['arguments'])\n",
    "print(\"\\nModel Response: \", results_arc['samples']['arc_easy'][0]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_arc['samples']['arc_easy'][0]['acc'])\n",
    "\n",
    "print(\"\\n--------------------\\nSample 2:\\n\")\n",
    "print(\"Answer Key: \", results_arc['samples']['arc_easy'][1]['doc']['answerKey'])\n",
    "print(\"\\nArgument: \", results_arc['samples']['arc_easy'][1]['arguments'])\n",
    "print(\"\\nModel Response: \", results_arc['samples']['arc_easy'][1]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_arc['samples']['arc_easy'][1]['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734549f-e328-430d-9426-ebc1c1c9f92a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2. bbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9582aeed-3808-4435-863e-361b5845c8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Results:\n",
      " {'bbh_zeroshot_causal_judgement': {'alias': 'bbh_zeroshot_causal_judgement', 'exact_match,strict-match': np.float64(0.0), 'exact_match_stderr,strict-match': 0.0, 'exact_match,flexible-extract': np.float64(0.2727272727272727), 'exact_match_stderr,flexible-extract': 0.032655509459413576}} \n",
      "\n",
      "--------------------\n",
      "Sample 1:\n",
      "\n",
      "Answer Key:  No\n",
      "\n",
      "Argument:  [('Answer questions about causal attribution.\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\\nOptions:\\n- Yes\\n- No\\nA:', {'until': ['</s>', 'Q:', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0})]\n",
      "\n",
      "Model Response:  [[' neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral']]\n",
      "\n",
      "Model accuracy for this question:  0.0\n",
      "--------------------\n",
      "Sample 2:\n",
      "\n",
      "Answer Key:  No\n",
      "\n",
      "Argument:  [(\"Answer questions about causal attribution.\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nLong ago, when John was only 17 years old, he got a job working for a large manufacturing company. He started out working on an assembly line for minimum wage, but after a few years at the company, he was given a choice between two line manager positions. He could stay in the woodwork division, which is where he was currently working. Or he could move to the plastics division. John was unsure what to do because he liked working in the woodwork division, but he also thought it might be worth trying something different. He finally decided to switch to the plastics division and try something new. For the last 30 years, John has worked as a production line supervisor in the plastics division. After the first year there, the plastics division was moved to a different building with more space. Unfortunately, through the many years he worked there, John was exposed to asbestos, a highly carcinogenic substance. Most of the plastics division was quite safe, but the small part in which John worked was exposed to asbestos fibers. And now, although John has never smoked a cigarette in his life and otherwise lives a healthy lifestyle, he has a highly progressed and incurable case of lung cancer at the age of 50. John had seen three cancer specialists, all of whom confirmed the worst: that, except for pain, John's cancer was untreatable and he was absolutely certain to die from it very soon (the doctors estimated no more than 2 months). Yesterday, while John was in the hospital for a routine medical appointment, a new nurse accidentally administered the wrong medication to him. John was allergic to the drug and he immediately went into shock and experienced cardiac arrest (a heart attack). Doctors attempted to resuscitate him but he died minutes after the medication was administered. Did John's job cause his premature death?\\nOptions:\\n- Yes\\n- No\\nA:\", {'until': ['</s>', 'Q:', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0})]\n",
      "\n",
      "Model Response:  [[' No\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral\\n\\nResponse: neutral']]\n",
      "\n",
      "Model accuracy for this question:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------\\nResults:\\n\", results_bbh['results'], \"\\n\")\n",
    "print(\"--------------------\\nSample 1:\\n\")\n",
    "print(\"Answer Key: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['doc']['target'])\n",
    "print(\"\\nArgument: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['arguments'])\n",
    "print(\"\\nModel Response: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][0]['exact_match'])\n",
    "\n",
    "print(\"--------------------\\nSample 2:\\n\")\n",
    "print(\"Answer Key: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['doc']['target'])\n",
    "print(\"\\nArgument: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['arguments'])\n",
    "print(\"\\nModel Response: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['resps'])\n",
    "print(\"\\nModel accuracy for this question: \", results_bbh['samples']['bbh_zeroshot_causal_judgement'][1]['exact_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43eaa6-5154-407c-9072-606e0bd08f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3. Multifin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83503641-5703-4a64-beea-71e2c5736659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06928838951310862"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_multifin_post['benchmark_accuracy_exact']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f35be-3f23-4576-8a7f-cf175d945a28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 4. Fin Phrasebank test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79208727-39b5-4b43-87c1-e3ce561ef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ran in sections above - copied over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24819f4e-5c2c-4097-b72c-81805dd1fa56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy-exact: 0.5288659793814433 \n",
      "\n",
      "accuracy-proximate: 0.663659793814433 \n",
      "\n",
      "Model:  {'full_text': ['Text: {`` BG Crane has been a strong partner for Hiab in Australia for many years . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': ['Text: {The aim is to increase sales by at least one fifth in 2006 . Choose one of the following words: positive, neutral, negative}', 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n",
      "Model:  {'full_text': ['Answer: neutral', 'Text'], 'model_answer': 'neutral'} \n",
      " Correct:  neutral \n",
      "\n",
      "Model:  {'full_text': [\"Text: {It delivers a different user experience and enables us to widen the market we can address . '' Choose one of the following words: positive, neutral, negative}\", 'Answer: neutral'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n",
      "Model:  {'full_text': ['Answer: neutral', 'Text'], 'model_answer': 'neutral'} \n",
      " Correct:  positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy-exact:\", results_['benchmark_accuracy_exact'], \"\\n\")\n",
    "print(\"accuracy-proximate:\", results_['benchmark_accuracy_proximate'], \"\\n\")\n",
    "for i in range(5): print(\"Model: \", results_['samples'][i]['formatted_response'], \"\\n\", \"Correct: \", results_['samples'][i]['correct_answer'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4354cb-edfb-4bf8-9638-456ad594a3f8",
   "metadata": {},
   "source": [
    "## Step 5: Interpretation of Results (20 points)\n",
    "In one-two paragraphs, summarize the results from training your model, noting how the outputs improved post training, how performance on the benchmarks and testing split changed post training, and whether and how the improvements and drawbacks from training you noticed empirically matched those you anticipated in Step 1 above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f63c0-3603-45c0-8a94-29b51607e312",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The prompt tuning of GPT2-instruct, combined with few shot prompting and prompt engineering, was moderately effective at enhancing the model's ability to classify financial news sentiment, using the 'best model' described above. The primary benchmark I used to assess this was the test set from the financial phrasebank dataset, over 900 rows. I created two metrics in the benchmark using this test data. One determined the model's accuracy in selecting the exact answer, and a second metric computed the model answer's proximity to the correct answer, which punished selecting the category on the opposite side of the scale - choosing positive if the correct answer is negative, and vice versa. Before prompt tuning, the exact-match accuracy was 0.25 and the proximity-accuracy was 0.46. Afterwards, the exact match increased to just over 50%, at 0.52, and the approximate accuracy increased 0.66, indicating that model was moderately effective at choosing close to the right answer. If we were to allow this sentiment classification to feed financial decisionmaking, after prompt tuning, we will have at least largely prevented the model from advising completely opposite of the right decision. \n",
    "\n",
    "I originally intended to use multifin as the specialized benchmark. It performed very poorly before prompt tuning and even slightly worse afterwards, probably because it had too many categories, so I decided to focus on the financial phrasebank test dataset as the primary benchmark. The two generalized benchmarks did not rate an accuracy much lower afterwards, roughly 5% lower for each metric, although when looking at the output it was clearly less coherent, resulting from some forgetting. Because the primary benchmark performed much better, I am okay with this. I only intend to use the model for financial sentiment analysis. That said, while I saw significant improvement in the model, increasing to above 50%, it is not ready to be deployed and feed decision making. The GPT2 instruct model, even with few shot prompt engineering, did not provide concise responses and required a function with a formatted one-word response. With more computing resources I would want to run the same model training approach, prompt engineering, benchmarks, and financial dataset on Llama 3-3B instruct. I intended to use Llama initially, although I struggled to find the computing resources needed and switched to GPT2. Because the training dataset included slightly under 4,000 rows, which should be large enough, it is worth exploring whether there is a second labeled dataset, with three sentiment categories but not necessarily finance-related, that could be added on to the financial phrasebank data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b9a44-af87-434d-922f-2ae84d67a9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
